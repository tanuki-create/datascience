# 非線形回帰

## 目次
1. [非線形回帰とは](#非線形回帰とは)
2. [多項式特徴量](#多項式特徴量)
3. [多項式回帰](#多項式回帰)
4. [k近傍法（kNN）回帰](#k近傍法knn回帰)
5. [線形 vs 非線形の比較](#線形-vs-非線形の比較)
6. [実装の比較](#実装の比較)
7. [まとめ](#まとめ)

## 非線形回帰とは

### 定義
非線形回帰（Nonlinear Regression）は、**目的変数と説明変数の間に非線形関係を仮定した回帰手法**です。線形回帰では捉えきれない複雑な関係をモデル化できます。

### 非線形回帰の必要性

#### 1. 線形回帰の限界
- **線形関係の仮定**: 実際のデータは非線形のことが多い
- **複雑な関係**: 単純な線形関係では説明できない
- **予測精度**: 非線形関係を考慮することで精度向上

#### 2. 非線形回帰の利点
- **柔軟性**: 複雑な関係をモデル化
- **精度向上**: より正確な予測
- **実用性**: 現実的な問題に対応

### 非線形回帰の種類

#### 1. 多項式回帰
- 多項式特徴量の生成
- 線形回帰の拡張
- 実装が簡単

#### 2. k近傍法（kNN）回帰
- 距離に基づく予測
- 局所的な関係を活用
- パラメータが少ない

#### 3. その他の手法
- スプライン回帰
- ガウス過程回帰
- ニューラルネットワーク

## 多項式特徴量

### 基本概念

**多項式特徴量**は、**既存の特徴量から高次の項を生成**する手法です。

### 数学的表現

#### 1. 単一特徴量の場合
```
x → x, x², x³, ..., xᵈ
```

#### 2. 複数特徴量の場合
```
x₁, x₂ → x₁, x₂, x₁², x₁x₂, x₂², x₁³, x₁²x₂, x₁x₂², x₂³, ...
```

### 多項式特徴量の生成

#### 1. 手動生成
```python
# 2次多項式特徴量の生成
X_poly = np.column_stack([
    X,           # 1次項
    X**2,        # 2次項
    X[:, 0:1] * X[:, 1:2]  # 交互作用項
])
```

#### 2. scikit-learnの活用
```python
from sklearn.preprocessing import PolynomialFeatures

# 2次多項式特徴量の生成
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X)
```

### 多項式特徴量の特徴

#### 1. 利点
- **非線形関係の表現**: 複雑な関係を捉える
- **実装が簡単**: 線形回帰の拡張
- **解釈可能**: 係数の意味が明確

#### 2. 欠点
- **特徴量の爆発**: 次元数が急激に増加
- **過学習のリスク**: 高次項で過学習しやすい
- **計算量の増加**: 特徴量数に比例して増加

### 次元数の計算

#### 1. 単一特徴量
```
次元数 = degree + 1
```

#### 2. 複数特徴量
```
次元数 = C(degree + n_features, degree)
```

ここで：
- **degree**: 多項式の次数
- **n_features**: 元の特徴量数
- **C**: 組み合わせ

### 具体例：次元数の計算

**例**: 2つの特徴量、3次多項式
- 元の特徴量数: 2
- 次数: 3
- 次元数: C(3+2, 3) = C(5, 3) = 10

**生成される特徴量**:
1. x₁ (1次項)
2. x₂ (1次項)
3. x₁² (2次項)
4. x₁x₂ (交互作用項)
5. x₂² (2次項)
6. x₁³ (3次項)
7. x₁²x₂ (3次項)
8. x₁x₂² (3次項)
9. x₂³ (3次項)
10. 定数項（bias）

## 多項式回帰

### 基本概念

**多項式回帰**は、**多項式特徴量を使用した線形回帰**です。

### 数学的表現

#### 1. 2次多項式回帰
```
y = β₀ + β₁x + β₂x² + ε
```

#### 2. 3次多項式回帰
```
y = β₀ + β₁x + β₂x² + β₃x³ + ε
```

#### 3. 複数特徴量の場合
```
y = β₀ + β₁x₁ + β₂x₂ + β₃x₁² + β₄x₁x₂ + β₅x₂² + ε
```

### 多項式回帰の実装

#### 1. 基本的な実装
```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline

# パイプラインの構築
poly_reg = Pipeline([
    ('poly', PolynomialFeatures(degree=2)),
    ('linear', LinearRegression())
])

# 学習と予測
poly_reg.fit(X, y)
y_pred = poly_reg.predict(X)
```

#### 2. 正則化の追加
```python
from sklearn.linear_model import Ridge

# Ridge回帰を使用
poly_reg = Pipeline([
    ('poly', PolynomialFeatures(degree=2)),
    ('ridge', Ridge(alpha=1.0))
])
```

### 多項式回帰の特徴

#### 1. 利点
- **非線形関係の表現**: 複雑な関係を捉える
- **実装が簡単**: 線形回帰の拡張
- **解釈可能**: 係数の意味が明確

#### 2. 欠点
- **過学習のリスク**: 高次項で過学習しやすい
- **外挿の困難**: 訓練範囲外での予測が不安定
- **計算量の増加**: 特徴量数に比例して増加

### 次数の選択

#### 1. 交差検証による選択
```python
from sklearn.model_selection import cross_val_score

# 異なる次数での性能比較
degrees = [1, 2, 3, 4, 5]
scores = []

for degree in degrees:
    poly_reg = Pipeline([
        ('poly', PolynomialFeatures(degree=degree)),
        ('linear', LinearRegression())
    ])
    score = cross_val_score(poly_reg, X, y, cv=5).mean()
    scores.append(score)
```

#### 2. 学習曲線の分析
- 訓練誤差 vs テスト誤差
- 過学習の検出
- 最適な次数の決定

## k近傍法（kNN）回帰

### 基本概念

**k近傍法（kNN）回帰**は、**距離に基づいて近傍のk個のサンプルから予測**する手法です。

### アルゴリズム

#### 1. 基本的な流れ
1. 新しいサンプルとの距離を計算
2. 最も近いk個のサンプルを選択
3. 選択されたサンプルの目的変数の平均を計算
4. 平均値を予測値として出力

#### 2. 距離の計算
```
ユークリッド距離: d = √(Σ(xᵢ - xⱼ)²)
マンハッタン距離: d = Σ|xᵢ - xⱼ|
```

### kNN回帰の実装

#### 1. 基本的な実装
```python
from sklearn.neighbors import KNeighborsRegressor

# kNN回帰の構築
knn_reg = KNeighborsRegressor(n_neighbors=5)

# 学習と予測
knn_reg.fit(X, y)
y_pred = knn_reg.predict(X)
```

#### 2. 距離の重み付け
```python
# 距離に基づく重み付け
knn_reg = KNeighborsRegressor(
    n_neighbors=5,
    weights='distance'
)
```

### kNN回帰の特徴

#### 1. 利点
- **非線形関係**: 複雑な関係を捉える
- **局所的な関係**: 近傍の関係を活用
- **パラメータが少ない**: kのみ調整
- **実装が簡単**: 距離計算のみ

#### 2. 欠点
- **計算量が大きい**: 全サンプルとの距離計算
- **次元の呪い**: 高次元では性能低下
- **外挿の困難**: 訓練範囲外での予測が不安定
- **解釈が困難**: 係数がない

### kの選択

#### 1. 交差検証による選択
```python
from sklearn.model_selection import cross_val_score

# 異なるkでの性能比較
k_values = [1, 3, 5, 7, 9, 11]
scores = []

for k in k_values:
    knn_reg = KNeighborsRegressor(n_neighbors=k)
    score = cross_val_score(knn_reg, X, y, cv=5).mean()
    scores.append(score)
```

#### 2. kの影響
- **kが小さい**: 局所的、過学習のリスク
- **kが大きい**: 滑らか、学習不足のリスク
- **最適なk**: 交差検証で決定

## 線形 vs 非線形の比較

### 性能の比較

#### 1. 線形関係のデータ
- **線形回帰**: 高い性能
- **多項式回帰**: 過学習のリスク
- **kNN回帰**: 計算量が大きい

#### 2. 非線形関係のデータ
- **線形回帰**: 低い性能
- **多項式回帰**: 高い性能
- **kNN回帰**: 高い性能

### 計算量の比較

| 手法 | 学習時間 | 予測時間 | メモリ使用量 |
|------|----------|----------|-------------|
| 線形回帰 | O(n) | O(1) | O(p) |
| 多項式回帰 | O(n) | O(1) | O(p²) |
| kNN回帰 | O(1) | O(n) | O(n) |

ここで：
- **n**: サンプル数
- **p**: 特徴量数

### 解釈性の比較

#### 1. 線形回帰
- **高い解釈性**: 係数の意味が明確
- **統計的推論**: 信頼区間、検定が可能
- **特徴量の重要度**: 係数の大きさで判断

#### 2. 多項式回帰
- **中程度の解釈性**: 係数の意味が複雑
- **統計的推論**: 可能だが複雑
- **特徴量の重要度**: 係数の組み合わせで判断

#### 3. kNN回帰
- **低い解釈性**: 係数がない
- **統計的推論**: 困難
- **特徴量の重要度**: 距離計算で判断

### 実装の比較

#### 1. 実装の複雑さ
- **線形回帰**: 最も簡単
- **多項式回帰**: 中程度
- **kNN回帰**: 中程度

#### 2. ハイパーパラメータ
- **線形回帰**: なし
- **多項式回帰**: 次数、正則化
- **kNN回帰**: k、距離関数

#### 3. 前処理の必要性
- **線形回帰**: スケーリング推奨
- **多項式回帰**: スケーリング必須
- **kNN回帰**: スケーリング必須

## 実装の比較

### 手法選択の指針

#### 1. データの特性
- **線形関係**: 線形回帰
- **非線形関係**: 多項式回帰、kNN回帰
- **局所的な関係**: kNN回帰
- **滑らかな関係**: 多項式回帰

#### 2. 計算リソース
- **計算量重視**: 線形回帰
- **メモリ重視**: 線形回帰、多項式回帰
- **予測速度重視**: 線形回帰、多項式回帰

#### 3. 解釈性
- **高い解釈性**: 線形回帰
- **中程度の解釈性**: 多項式回帰
- **低い解釈性**: kNN回帰

### 実装時の注意点

#### 1. データの前処理
- **スケーリング**: 多項式回帰、kNN回帰で必須
- **外れ値**: 全手法で影響を受ける
- **欠損値**: 適切な処理が必要

#### 2. 過学習の回避
- **正則化**: 多項式回帰で有効
- **交差検証**: 全手法で有効
- **特徴選択**: 多項式回帰で有効

#### 3. 性能評価
- **複数指標**: MSE, RMSE, MAE, R²
- **可視化**: 残差プロット、予測結果
- **交差検証**: 汎化性能の確認

## まとめ

### 非線形回帰の重要性

非線形回帰は、現実の複雑な関係をモデル化するために不可欠です。適切な手法の選択により、より正確な予測が可能になります。

### 主要な手法

- **多項式回帰**: 滑らかな非線形関係
- **kNN回帰**: 局所的な非線形関係
- **その他**: スプライン、ガウス過程等

### 実践的なポイント

1. **データの理解**: 関係の性質を把握
2. **手法の選択**: 問題に適した手法
3. **過学習の回避**: 正則化、交差検証
4. **性能評価**: 複数指標での評価

### 次のステップ

- [総まとめ](../07_summary/07_comprehensive_summary.md)

---

**関連ノートブック**:
- [多項式特徴量](./notebooks/polynomial_features.ipynb)
- [多項式回帰](./notebooks/polynomial_regression.ipynb)
- [kNN回帰](./notebooks/knn_regression.ipynb)
- [様々なkの比較](./notebooks/knn_various_k.ipynb)
