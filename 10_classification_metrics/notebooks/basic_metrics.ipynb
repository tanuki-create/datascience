{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 基本評価指標の実装\n",
        "\n",
        "このノートブックでは、分類問題の基本評価指標（Accuracy、Precision、Recall、Specificity）を実装し、理解を深めます。\n",
        "\n",
        "## 学習目標\n",
        "- 基本評価指標の理論的理解\n",
        "- 手動実装とscikit-learnの比較\n",
        "- 実データでの指標計算\n",
        "- 指標の解釈と活用\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 必要なライブラリのインポート\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification, load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
        "                           f1_score, confusion_matrix, classification_report)\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 日本語フォントの設定\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "sns.set_style(\"whitegrid\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 基本評価指標の手動実装\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_metrics_manual(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    基本評価指標の手動実装\n",
        "    \n",
        "    Parameters:\n",
        "    y_true: 実際のラベル\n",
        "    y_pred: 予測ラベル\n",
        "    \n",
        "    Returns:\n",
        "    metrics: 評価指標の辞書\n",
        "    \"\"\"\n",
        "    # 混同行列の計算\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    \n",
        "    if cm.shape == (2, 2):\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "        \n",
        "        # 基本指標の計算\n",
        "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        \n",
        "        return {\n",
        "            'Accuracy': accuracy,\n",
        "            'Precision': precision,\n",
        "            'Recall': recall,\n",
        "            'Specificity': specificity,\n",
        "            'F1-score': f1,\n",
        "            'TP': tp,\n",
        "            'TN': tn,\n",
        "            'FP': fp,\n",
        "            'FN': fn\n",
        "        }\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# 簡単な例で指標を計算\n",
        "y_true_example = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0])\n",
        "y_pred_example = np.array([1, 0, 0, 1, 0, 1, 1, 0, 1, 0])\n",
        "\n",
        "metrics_manual = calculate_metrics_manual(y_true_example, y_pred_example)\n",
        "\n",
        "print(\"=== 基本評価指標の手動実装 ===\")\n",
        "print(f\"実際のラベル: {y_true_example}\")\n",
        "print(f\"予測ラベル: {y_pred_example}\")\n",
        "print(f\"\\n混同行列:\")\n",
        "cm_example = confusion_matrix(y_true_example, y_pred_example)\n",
        "print(cm_example)\n",
        "\n",
        "if metrics_manual:\n",
        "    print(f\"\\n計算された指標:\")\n",
        "    for metric, value in metrics_manual.items():\n",
        "        if isinstance(value, float):\n",
        "            print(f\"{metric}: {value:.4f}\")\n",
        "        else:\n",
        "            print(f\"{metric}: {value}\")\n",
        "\n",
        "# scikit-learnとの比較\n",
        "accuracy_sklearn = accuracy_score(y_true_example, y_pred_example)\n",
        "precision_sklearn = precision_score(y_true_example, y_pred_example)\n",
        "recall_sklearn = recall_score(y_true_example, y_pred_example)\n",
        "f1_sklearn = f1_score(y_true_example, y_pred_example)\n",
        "\n",
        "print(f\"\\n=== scikit-learnとの比較 ===\")\n",
        "print(f\"Accuracy: 手動={metrics_manual['Accuracy']:.4f}, sklearn={accuracy_sklearn:.4f}\")\n",
        "print(f\"Precision: 手動={metrics_manual['Precision']:.4f}, sklearn={precision_sklearn:.4f}\")\n",
        "print(f\"Recall: 手動={metrics_manual['Recall']:.4f}, sklearn={recall_sklearn:.4f}\")\n",
        "print(f\"F1-score: 手動={metrics_manual['F1-score']:.4f}, sklearn={f1_sklearn:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 指標の可視化と解釈\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_metrics_comparison(metrics_dict, title=\"Evaluation Metrics\"):\n",
        "    \"\"\"\n",
        "    評価指標の可視化\n",
        "    \n",
        "    Parameters:\n",
        "    metrics_dict: 評価指標の辞書\n",
        "    title: グラフのタイトル\n",
        "    \"\"\"\n",
        "    # 数値指標のみを抽出\n",
        "    numeric_metrics = {k: v for k, v in metrics_dict.items() if isinstance(v, float)}\n",
        "    \n",
        "    # 可視化\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    \n",
        "    # バープロット\n",
        "    metrics_names = list(numeric_metrics.keys())\n",
        "    metrics_values = list(numeric_metrics.values())\n",
        "    \n",
        "    bars = ax1.bar(metrics_names, metrics_values, color=['skyblue', 'lightcoral', 'lightgreen', 'gold', 'plum'])\n",
        "    ax1.set_title(f'{title} - Bar Plot')\n",
        "    ax1.set_ylabel('Score')\n",
        "    ax1.set_ylim(0, 1)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 数値の表示\n",
        "    for bar, value in zip(bars, metrics_values):\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                f'{value:.3f}', ha='center', va='bottom')\n",
        "    \n",
        "    # レーダーチャート\n",
        "    angles = np.linspace(0, 2 * np.pi, len(metrics_names), endpoint=False).tolist()\n",
        "    angles += angles[:1]  # 閉じるため\n",
        "    \n",
        "    values = list(metrics_values) + [metrics_values[0]]  # 閉じるため\n",
        "    \n",
        "    ax2 = plt.subplot(122, projection='polar')\n",
        "    ax2.plot(angles, values, 'o-', linewidth=2, color='blue')\n",
        "    ax2.fill(angles, values, alpha=0.25, color='blue')\n",
        "    ax2.set_xticks(angles[:-1])\n",
        "    ax2.set_xticklabels(metrics_names)\n",
        "    ax2.set_ylim(0, 1)\n",
        "    ax2.set_title(f'{title} - Radar Chart')\n",
        "    ax2.grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 指標の可視化\n",
        "if metrics_manual:\n",
        "    plot_metrics_comparison(metrics_manual, \"Basic Evaluation Metrics\")\n",
        "\n",
        "# 指標の解釈\n",
        "print(\"=== 指標の解釈 ===\")\n",
        "print(\"Accuracy (精度): 全予測のうち正解した割合\")\n",
        "print(\"Precision (適合率): 正例と予測したもののうち、実際に正例だった割合\")\n",
        "print(\"Recall (再現率): 実際の正例のうち、正しく正例と予測できた割合\")\n",
        "print(\"Specificity (特異度): 実際の負例のうち、正しく負例と予測できた割合\")\n",
        "print(\"F1-score: PrecisionとRecallの調和平均\")\n",
        "\n",
        "# 実務での意味\n",
        "print(\"\\n=== 実務での意味 ===\")\n",
        "print(\"医療診断の例:\")\n",
        "print(\"- Precision: 病気と診断した患者のうち、実際に病気だった割合\")\n",
        "print(\"- Recall: 病気の患者のうち、正しく病気と診断できた割合\")\n",
        "print(\"- Specificity: 健康な患者のうち、正しく健康と診断できた割合\")\n",
        "\n",
        "print(\"\\nスパム判定の例:\")\n",
        "print(\"- Precision: スパムと判定したメールのうち、実際にスパムだった割合\")\n",
        "print(\"- Recall: スパムメールのうち、正しくスパムと判定できた割合\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 実データでの評価指標\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 乳がんデータセットでの評価\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# データの分割と標準化\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ロジスティック回帰モデルの訓練\n",
        "model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 予測\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# 評価指標の計算\n",
        "metrics_cancer = calculate_metrics_manual(y_test, y_pred)\n",
        "\n",
        "print(\"=== 乳がんデータセットでの評価 ===\")\n",
        "print(f\"データセットの情報:\")\n",
        "print(f\"  サンプル数: {len(y_test)}\")\n",
        "print(f\"  クラス分布: {np.bincount(y_test)}\")\n",
        "print(f\"  クラス名: {cancer.target_names}\")\n",
        "\n",
        "if metrics_cancer:\n",
        "    print(f\"\\n評価指標:\")\n",
        "    for metric, value in metrics_cancer.items():\n",
        "        if isinstance(value, float):\n",
        "            print(f\"  {metric}: {value:.4f}\")\n",
        "        else:\n",
        "            print(f\"  {metric}: {value}\")\n",
        "\n",
        "# 混同行列の可視化\n",
        "cm_cancer = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_cancer, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Malignant', 'Benign'],\n",
        "            yticklabels=['Malignant', 'Benign'])\n",
        "plt.title('Confusion Matrix - Breast Cancer Dataset')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n",
        "\n",
        "# 分類レポート\n",
        "print(\"\\n=== 詳細な分類レポート ===\")\n",
        "print(classification_report(y_test, y_pred, target_names=cancer.target_names))\n",
        "\n",
        "# 指標の可視化\n",
        "if metrics_cancer:\n",
        "    plot_metrics_comparison(metrics_cancer, \"Breast Cancer Classification Metrics\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 閾値の影響\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_thresholds(y_true, y_pred_proba, thresholds):\n",
        "    \"\"\"\n",
        "    異なる閾値での評価指標を計算\n",
        "    \n",
        "    Parameters:\n",
        "    y_true: 実際のラベル\n",
        "    y_pred_proba: 予測確率\n",
        "    thresholds: 閾値のリスト\n",
        "    \n",
        "    Returns:\n",
        "    results: 各閾値での評価指標\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for threshold in thresholds:\n",
        "        y_pred_thresh = (y_pred_proba >= threshold).astype(int)\n",
        "        metrics = calculate_metrics_manual(y_true, y_pred_thresh)\n",
        "        \n",
        "        if metrics:\n",
        "            metrics['Threshold'] = threshold\n",
        "            results.append(metrics)\n",
        "    \n",
        "    return results\n",
        "\n",
        "# 異なる閾値での評価\n",
        "thresholds = np.arange(0.1, 1.0, 0.1)\n",
        "threshold_results = evaluate_thresholds(y_test, y_pred_proba, thresholds)\n",
        "\n",
        "# 結果の可視化\n",
        "if threshold_results:\n",
        "    # データフレームの作成\n",
        "    df_thresholds = pd.DataFrame(threshold_results)\n",
        "    \n",
        "    # 指標の可視化\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Precision vs Threshold\n",
        "    axes[0, 0].plot(df_thresholds['Threshold'], df_thresholds['Precision'], 'o-', color='blue')\n",
        "    axes[0, 0].set_title('Precision vs Threshold')\n",
        "    axes[0, 0].set_xlabel('Threshold')\n",
        "    axes[0, 0].set_ylabel('Precision')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Recall vs Threshold\n",
        "    axes[0, 1].plot(df_thresholds['Threshold'], df_thresholds['Recall'], 'o-', color='red')\n",
        "    axes[0, 1].set_title('Recall vs Threshold')\n",
        "    axes[0, 1].set_xlabel('Threshold')\n",
        "    axes[0, 1].set_ylabel('Recall')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # F1-score vs Threshold\n",
        "    axes[1, 0].plot(df_thresholds['Threshold'], df_thresholds['F1-score'], 'o-', color='green')\n",
        "    axes[1, 0].set_title('F1-score vs Threshold')\n",
        "    axes[1, 0].set_xlabel('Threshold')\n",
        "    axes[1, 0].set_ylabel('F1-score')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Precision vs Recall\n",
        "    axes[1, 1].plot(df_thresholds['Recall'], df_thresholds['Precision'], 'o-', color='purple')\n",
        "    axes[1, 1].set_title('Precision vs Recall')\n",
        "    axes[1, 1].set_xlabel('Recall')\n",
        "    axes[1, 1].set_ylabel('Precision')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # 最適な閾値の選択\n",
        "    best_f1_idx = df_thresholds['F1-score'].idxmax()\n",
        "    best_threshold = df_thresholds.loc[best_f1_idx, 'Threshold']\n",
        "    best_f1 = df_thresholds.loc[best_f1_idx, 'F1-score']\n",
        "    \n",
        "    print(f\"=== 閾値の影響分析 ===\")\n",
        "    print(f\"最適なF1-score: {best_f1:.4f} (閾値: {best_threshold:.1f})\")\n",
        "    print(f\"\\n各閾値での指標:\")\n",
        "    print(df_thresholds[['Threshold', 'Precision', 'Recall', 'F1-score']].round(4))\n",
        "    \n",
        "    # 閾値の解釈\n",
        "    print(f\"\\n=== 閾値の解釈 ===\")\n",
        "    print(\"低い閾値 (0.1-0.3):\")\n",
        "    print(\"  - Recall ↑, Precision ↓\")\n",
        "    print(\"  - より多くの正例を検出\")\n",
        "    print(\"  - 偽陽性が増加\")\n",
        "    \n",
        "    print(\"\\n高い閾値 (0.7-0.9):\")\n",
        "    print(\"  - Precision ↑, Recall ↓\")\n",
        "    print(\"  - より確実な正例のみ検出\")\n",
        "    print(\"  - 偽陰性が増加\")\n",
        "    \n",
        "    print(f\"\\n最適な閾値 ({best_threshold:.1f}):\")\n",
        "    print(\"  - PrecisionとRecallのバランス\")\n",
        "    print(\"  - F1-scoreが最大\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 演習問題\n",
        "\n",
        "### 演習1: 指標の手動実装\n",
        "異なるデータセットで基本評価指標を手動実装し、scikit-learnの結果と比較してみましょう。\n",
        "\n",
        "### 演習2: 閾値の最適化\n",
        "医療診断ではRecallを重視し、スパム判定ではPrecisionを重視する場合の最適な閾値を探してみましょう。\n",
        "\n",
        "### 演習3: クラス不均衡の影響\n",
        "クラス不均衡データで基本評価指標を計算し、指標の変化を観察してみましょう。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## まとめ\n",
        "\n",
        "このノートブックでは、分類問題の基本評価指標を詳しく学習しました。\n",
        "\n",
        "**学習した内容**：\n",
        "- 基本評価指標の理論と実装\n",
        "- 手動実装とscikit-learnの比較\n",
        "- 実データでの指標計算と解釈\n",
        "- 閾値の影響と最適化\n",
        "\n",
        "**重要なポイント**：\n",
        "- Accuracy、Precision、Recall、Specificity、F1-scoreの理解\n",
        "- 各指標の実務での意味\n",
        "- 閾値の調整による指標の変化\n",
        "- 可視化による直感的な理解\n",
        "\n",
        "**次のステップ**：\n",
        "- Precision-Recall曲線の学習\n",
        "- ROC曲線とAUCの理解\n",
        "- 多クラス分類の評価指標\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
