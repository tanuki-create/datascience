{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ロジスティック回帰の実装\n",
        "\n",
        "このノートブックでは、ロジスティック回帰をスクラッチから実装し、scikit-learnとの比較を行います。\n",
        "\n",
        "## 学習目標\n",
        "- ロジスティック回帰の理論的理解\n",
        "- シグモイド関数の実装\n",
        "- 交差エントロピー損失の計算\n",
        "- 勾配降下法による最適化\n",
        "- 決定境界の可視化\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 必要なライブラリのインポート\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 日本語フォントの設定\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "sns.set_style(\"whitegrid\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. データの準備\n",
        "\n",
        "まず、分類用のデータセットを生成します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# データセットの生成\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=2,\n",
        "    n_redundant=0,\n",
        "    n_informative=2,\n",
        "    n_clusters_per_class=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# データの可視化\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X[y == 0, 0], X[y == 0, 1], c='red', label='Class 0', alpha=0.7)\n",
        "plt.scatter(X[y == 1, 0], X[y == 1, 1], c='blue', label='Class 1', alpha=0.7)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('Generated Classification Dataset')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(f\"データセットの形状: {X.shape}\")\n",
        "print(f\"クラスの分布: {np.bincount(y)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# データの分割と標準化\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 特徴量の標準化\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"訓練データの形状: {X_train_scaled.shape}\")\n",
        "print(f\"テストデータの形状: {X_test_scaled.shape}\")\n",
        "print(f\"訓練データの平均: {X_train_scaled.mean(axis=0)}\")\n",
        "print(f\"訓練データの標準偏差: {X_train_scaled.std(axis=0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. ロジスティック回帰の実装\n",
        "\n",
        "### 2.1 シグモイド関数の実装\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    シグモイド関数の実装\n",
        "    \n",
        "    Parameters:\n",
        "    z: 線形結合の結果\n",
        "    \n",
        "    Returns:\n",
        "    sigmoid(z): シグモイド関数の値\n",
        "    \"\"\"\n",
        "    # 数値的安定性のため、zが大きすぎる場合は制限\n",
        "    z = np.clip(z, -500, 500)\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# シグモイド関数の可視化\n",
        "z = np.linspace(-10, 10, 100)\n",
        "sigmoid_values = sigmoid(z)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(z, sigmoid_values, 'b-', linewidth=2, label='Sigmoid Function')\n",
        "plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.7, label='Decision Threshold (0.5)')\n",
        "plt.axvline(x=0, color='g', linestyle='--', alpha=0.7, label='z = 0')\n",
        "plt.xlabel('z')\n",
        "plt.ylabel('σ(z)')\n",
        "plt.title('Sigmoid Function')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(\"シグモイド関数の特徴:\")\n",
        "print(f\"σ(0) = {sigmoid(0):.3f}\")\n",
        "print(f\"σ(5) = {sigmoid(5):.3f}\")\n",
        "print(f\"σ(-5) = {sigmoid(-5):.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 交差エントロピー損失の実装\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    交差エントロピー損失の計算\n",
        "    \n",
        "    Parameters:\n",
        "    y_true: 実際のラベル\n",
        "    y_pred: 予測確率\n",
        "    \n",
        "    Returns:\n",
        "    loss: 交差エントロピー損失\n",
        "    \"\"\"\n",
        "    # 数値的安定性のため、予測値をクリップ\n",
        "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "    \n",
        "    # 交差エントロピー損失の計算\n",
        "    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "    return loss\n",
        "\n",
        "# 損失関数の可視化\n",
        "y_true_example = np.array([1, 0, 1, 0])\n",
        "y_pred_example = np.linspace(0.01, 0.99, 100)\n",
        "\n",
        "losses = []\n",
        "for pred in y_pred_example:\n",
        "    loss = cross_entropy_loss(y_true_example, np.full_like(y_true_example, pred))\n",
        "    losses.append(loss)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(y_pred_example, losses, 'b-', linewidth=2)\n",
        "plt.xlabel('Predicted Probability')\n",
        "plt.ylabel('Cross-Entropy Loss')\n",
        "plt.title('Cross-Entropy Loss vs Predicted Probability')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(\"交差エントロピー損失の特徴:\")\n",
        "print(f\"y_true=1, y_pred=0.9 の損失: {cross_entropy_loss([1], [0.9]):.3f}\")\n",
        "print(f\"y_true=1, y_pred=0.1 の損失: {cross_entropy_loss([1], [0.1]):.3f}\")\n",
        "print(f\"y_true=0, y_pred=0.1 の損失: {cross_entropy_loss([0], [0.1]):.3f}\")\n",
        "print(f\"y_true=0, y_pred=0.9 の損失: {cross_entropy_loss([0], [0.9]):.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 ロジスティック回帰クラスの実装\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LogisticRegressionScratch:\n",
        "    \"\"\"\n",
        "    ロジスティック回帰のスクラッチ実装\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, learning_rate=0.01, max_iterations=1000, tolerance=1e-6):\n",
        "        \"\"\"\n",
        "        パラメータの初期化\n",
        "        \n",
        "        Parameters:\n",
        "        learning_rate: 学習率\n",
        "        max_iterations: 最大反復回数\n",
        "        tolerance: 収束判定の閾値\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.cost_history = []\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        モデルの訓練\n",
        "        \n",
        "        Parameters:\n",
        "        X: 特徴量\n",
        "        y: ラベル\n",
        "        \"\"\"\n",
        "        # パラメータの初期化\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "        \n",
        "        # 勾配降下法\n",
        "        for i in range(self.max_iterations):\n",
        "            # 線形結合の計算\n",
        "            z = np.dot(X, self.weights) + self.bias\n",
        "            \n",
        "            # シグモイド関数の適用\n",
        "            predictions = sigmoid(z)\n",
        "            \n",
        "            # 損失の計算\n",
        "            cost = cross_entropy_loss(y, predictions)\n",
        "            self.cost_history.append(cost)\n",
        "            \n",
        "            # 勾配の計算\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (predictions - y))\n",
        "            db = (1 / n_samples) * np.sum(predictions - y)\n",
        "            \n",
        "            # パラメータの更新\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "            \n",
        "            # 収束判定\n",
        "            if i > 0 and abs(self.cost_history[-1] - self.cost_history[-2]) < self.tolerance:\n",
        "                print(f\"収束しました。反復回数: {i+1}\")\n",
        "                break\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        確率の予測\n",
        "        \n",
        "        Parameters:\n",
        "        X: 特徴量\n",
        "        \n",
        "        Returns:\n",
        "        probabilities: 予測確率\n",
        "        \"\"\"\n",
        "        z = np.dot(X, self.weights) + self.bias\n",
        "        return sigmoid(z)\n",
        "    \n",
        "    def predict(self, X, threshold=0.5):\n",
        "        \"\"\"\n",
        "        クラスの予測\n",
        "        \n",
        "        Parameters:\n",
        "        X: 特徴量\n",
        "        threshold: 決定閾値\n",
        "        \n",
        "        Returns:\n",
        "        predictions: 予測クラス\n",
        "        \"\"\"\n",
        "        probabilities = self.predict_proba(X)\n",
        "        return (probabilities >= threshold).astype(int)\n",
        "    \n",
        "    def score(self, X, y):\n",
        "        \"\"\"\n",
        "        精度の計算\n",
        "        \n",
        "        Parameters:\n",
        "        X: 特徴量\n",
        "        y: ラベル\n",
        "        \n",
        "        Returns:\n",
        "        accuracy: 精度\n",
        "        \"\"\"\n",
        "        predictions = self.predict(X)\n",
        "        return accuracy_score(y, predictions)\n",
        "\n",
        "# モデルの訓練\n",
        "model = LogisticRegressionScratch(learning_rate=0.1, max_iterations=1000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 損失の履歴を可視化\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(model.cost_history)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Cost')\n",
        "plt.title('Cost History During Training')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(f\"最終的な損失: {model.cost_history[-1]:.4f}\")\n",
        "print(f\"重み: {model.weights}\")\n",
        "print(f\"バイアス: {model.bias:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 決定境界の可視化\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_decision_boundary(model, X, y, title=\"Decision Boundary\"):\n",
        "    \"\"\"\n",
        "    決定境界の可視化\n",
        "    \n",
        "    Parameters:\n",
        "    model: 訓練済みモデル\n",
        "    X: 特徴量\n",
        "    y: ラベル\n",
        "    title: グラフのタイトル\n",
        "    \"\"\"\n",
        "    # メッシュグリッドの作成\n",
        "    h = 0.01\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    \n",
        "    # 予測\n",
        "    Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    # プロット\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.contourf(xx, yy, Z, levels=50, alpha=0.8, cmap='RdYlBu')\n",
        "    plt.colorbar(label='Predicted Probability')\n",
        "    \n",
        "    # データポイントのプロット\n",
        "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', edgecolors='black', linewidth=1)\n",
        "    plt.colorbar(scatter, label='True Class')\n",
        "    \n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.title(title)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "# 決定境界の可視化\n",
        "plot_decision_boundary(model, X_train_scaled, y_train, \"Training Data - Decision Boundary\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. モデルの評価\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 予測の実行\n",
        "y_pred_train = model.predict(X_train_scaled)\n",
        "y_pred_test = model.predict(X_test_scaled)\n",
        "y_pred_proba_test = model.predict_proba(X_test_scaled)\n",
        "\n",
        "# 精度の計算\n",
        "train_accuracy = model.score(X_train_scaled, y_train)\n",
        "test_accuracy = model.score(X_test_scaled, y_test)\n",
        "\n",
        "print(\"=== モデルの評価結果 ===\")\n",
        "print(f\"訓練データの精度: {train_accuracy:.4f}\")\n",
        "print(f\"テストデータの精度: {test_accuracy:.4f}\")\n",
        "\n",
        "# 混同行列の表示\n",
        "print(\"\\n=== 混同行列（テストデータ） ===\")\n",
        "cm = confusion_matrix(y_test, y_pred_test)\n",
        "print(cm)\n",
        "\n",
        "# 分類レポートの表示\n",
        "print(\"\\n=== 分類レポート（テストデータ） ===\")\n",
        "print(classification_report(y_test, y_pred_test))\n",
        "\n",
        "# 予測確率の分布\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(y_pred_proba_test[y_test == 0], bins=20, alpha=0.7, label='Class 0', color='red')\n",
        "plt.hist(y_pred_proba_test[y_test == 1], bins=20, alpha=0.7, label='Class 1', color='blue')\n",
        "plt.xlabel('Predicted Probability')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Predicted Probabilities')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(y_pred_proba_test, y_test, alpha=0.6)\n",
        "plt.xlabel('Predicted Probability')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Predicted Probability vs True Label')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. scikit-learnとの比較\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# scikit-learnのロジスティック回帰\n",
        "sklearn_model = LogisticRegression(random_state=42)\n",
        "sklearn_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 予測\n",
        "sklearn_pred_test = sklearn_model.predict(X_test_scaled)\n",
        "sklearn_proba_test = sklearn_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# 精度の比較\n",
        "sklearn_accuracy = sklearn_model.score(X_test_scaled, y_test)\n",
        "\n",
        "print(\"=== scikit-learnとの比較 ===\")\n",
        "print(f\"自作モデルの精度: {test_accuracy:.4f}\")\n",
        "print(f\"scikit-learnの精度: {sklearn_accuracy:.4f}\")\n",
        "print(f\"精度の差: {abs(test_accuracy - sklearn_accuracy):.4f}\")\n",
        "\n",
        "# 係数の比較\n",
        "print(f\"\\n=== 係数の比較 ===\")\n",
        "print(f\"自作モデルの重み: {model.weights}\")\n",
        "print(f\"scikit-learnの重み: {sklearn_model.coef_[0]}\")\n",
        "print(f\"自作モデルのバイアス: {model.bias:.4f}\")\n",
        "print(f\"scikit-learnのバイアス: {sklearn_model.intercept_[0]:.4f}\")\n",
        "\n",
        "# 予測確率の比較\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(model.predict_proba(X_test_scaled), sklearn_proba_test, alpha=0.6)\n",
        "plt.plot([0, 1], [0, 1], 'r--', alpha=0.8)\n",
        "plt.xlabel('自作モデルの予測確率')\n",
        "plt.ylabel('scikit-learnの予測確率')\n",
        "plt.title('予測確率の比較')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(model.predict_proba(X_test_scaled), sklearn_proba_test, alpha=0.6)\n",
        "plt.plot([0, 1], [0, 1], 'r--', alpha=0.8)\n",
        "plt.xlabel('自作モデルの予測確率')\n",
        "plt.ylabel('scikit-learnの予測確率')\n",
        "plt.title('予測確率の比較（拡大）')\n",
        "plt.xlim(0, 1)\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 演習問題\n",
        "\n",
        "### 演習1: 学習率の影響\n",
        "異なる学習率でモデルを訓練し、収束の様子を比較してみましょう。\n",
        "\n",
        "### 演習2: 正則化の実装\n",
        "L2正則化を追加して、過学習を防ぐ実装を作成してみましょう。\n",
        "\n",
        "### 演習3: 決定閾値の調整\n",
        "決定閾値を変更して、PrecisionとRecallのトレードオフを確認してみましょう。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## まとめ\n",
        "\n",
        "このノートブックでは、ロジスティック回帰をスクラッチから実装しました。\n",
        "\n",
        "**学習した内容**：\n",
        "- シグモイド関数の実装と可視化\n",
        "- 交差エントロピー損失の計算\n",
        "- 勾配降下法による最適化\n",
        "- 決定境界の可視化\n",
        "- scikit-learnとの比較\n",
        "\n",
        "**重要なポイント**：\n",
        "- ロジスティック回帰は確率的な出力を提供\n",
        "- 線形決定境界を持つ\n",
        "- 数値的安定性に注意が必要\n",
        "- 特徴量の標準化が重要\n",
        "\n",
        "**次のステップ**：\n",
        "- 多クラス分類への拡張\n",
        "- 正則化の実装\n",
        "- より高度な最適化手法の学習\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
