# ロジスティック回帰

## 概要

ロジスティック回帰は、分類問題を解決するための統計的手法です。線形回帰とは異なり、出力が確率（0から1の間）になるように設計されており、二値分類や多クラス分類に広く使用されています。

## 1. 分類タスクとは

### 1.1 回帰と分類の違い

**回帰問題**：
- 目的：連続値を予測
- 例：価格予測、売上予測、温度予測
- 出力：実数値

**分類問題**：
- 目的：カテゴリやクラスを予測
- 例：スパム判定、病気診断、画像認識
- 出力：離散値（カテゴリ）

### 1.2 分類の種類

**二値分類（Binary Classification）**：
- 2つのクラスに分類
- 例：スパム/非スパム、病気/健康、合格/不合格

**多クラス分類（Multi-class Classification）**：
- 3つ以上のクラスに分類
- 例：手書き数字認識（0-9）、動物分類（犬、猫、鳥）

## 2. ロジスティック回帰の理論

### 2.1 基本概念

ロジスティック回帰は、線形回帰の出力を確率に変換するために、シグモイド関数（ロジスティック関数）を使用します。

### 2.2 シグモイド関数

シグモイド関数は以下の式で定義されます：

```
σ(z) = 1 / (1 + e^(-z))
```

**特徴**：
- 出力範囲：[0, 1]
- S字カーブ
- z = 0のとき、σ(0) = 0.5
- z → +∞のとき、σ(z) → 1
- z → -∞のとき、σ(z) → 0

### 2.3 ロジスティック回帰の数式

**確率の予測**：
```
P(y = 1 | x) = σ(w₀ + w₁x₁ + w₂x₂ + ... + wₙxₙ)
```

**線形結合**：
```
z = w₀ + w₁x₁ + w₂x₂ + ... + wₙxₙ
```

**最終的な確率**：
```
P(y = 1 | x) = 1 / (1 + e^(-z))
```

### 2.4 決定境界

**決定境界**：
- P(y = 1 | x) = 0.5のときの境界
- つまり、z = 0のとき
- w₀ + w₁x₁ + w₂x₂ + ... + wₙxₙ = 0

**分類ルール**：
- P(y = 1 | x) > 0.5 → クラス1に分類
- P(y = 1 | x) < 0.5 → クラス0に分類

## 3. 損失関数

### 3.1 交差エントロピー損失（Cross-Entropy Loss）

ロジスティック回帰では、交差エントロピー損失を使用します：

**二値分類の場合**：
```
L = -[y·log(p) + (1-y)·log(1-p)]
```

ここで：
- y：実際のラベル（0または1）
- p：予測確率 P(y = 1 | x)

### 3.2 損失関数の直感的理解

**y = 1の場合**：
- 正解が1のとき、pが1に近いほど損失が小さい
- pが0に近いと、log(p)が-∞に近づき、損失が大きくなる

**y = 0の場合**：
- 正解が0のとき、pが0に近いほど損失が小さい
- pが1に近いと、log(1-p)が-∞に近づき、損失が大きくなる

### 3.3 平均損失

**訓練データ全体での平均損失**：
```
J(w) = -(1/m) Σ[i=1 to m] [y⁽ⁱ⁾·log(p⁽ⁱ⁾) + (1-y⁽ⁱ⁾)·log(1-p⁽ⁱ⁾)]
```

## 4. 最適化アルゴリズム

### 4.1 勾配降下法

**勾配の計算**：
```
∂J/∂wⱼ = (1/m) Σ[i=1 to m] (p⁽ⁱ⁾ - y⁽ⁱ⁾)xⱼ⁽ⁱ⁾
```

**パラメータの更新**：
```
wⱼ := wⱼ - α·∂J/∂wⱼ
```

### 4.2 正規方程式（解析解）

ロジスティック回帰には解析解が存在しないため、数値的最適化が必要です。

## 5. 実装の詳細

### 5.1 データの前処理

**特徴量の標準化**：
- ロジスティック回帰は特徴量のスケールに敏感
- StandardScalerやMinMaxScalerの使用を推奨

**カテゴリ変数の処理**：
- One-hot encoding
- Label encoding（順序がある場合）

### 5.2 ハイパーパラメータ

**学習率（α）**：
- 大きすぎる：発散する可能性
- 小さすぎる：収束が遅い
- 一般的な範囲：0.01～0.1

**正則化**：
- L1正則化（Lasso）：特徴選択効果
- L2正則化（Ridge）：過学習防止
- Elastic Net：L1とL2の組み合わせ

### 5.3 収束判定

**収束条件**：
- 勾配のノルムが閾値以下
- 損失の変化が閾値以下
- 最大反復回数に到達

## 6. 実務での応用

### 6.1 ビジネス応用例

**顧客離脱予測**：
- 特徴量：利用履歴、支払い履歴、サポート問い合わせ
- 目的：離脱リスクの高い顧客を特定

**信用リスク評価**：
- 特徴量：年収、借入履歴、職業
- 目的：融資承認の判定

**マーケティング**：
- 特徴量：デモグラフィック、行動履歴
- 目的：商品購入確率の予測

### 6.2 医療応用

**疾病診断**：
- 特徴量：検査値、症状、既往歴
- 目的：病気の有無の判定

**薬効予測**：
- 特徴量：遺伝子情報、症状
- 目的：薬の効果の予測

## 7. よくある落とし穴と注意点

### 7.1 データの不均衡

**問題**：
- クラス間のサンプル数の偏り
- 少数クラスの予測精度が低下

**対策**：
- SMOTE（合成少数オーバーサンプリング）
- クラス重みの調整
- 評価指標の選択（Precision、Recall、F1-score）

### 7.2 過学習

**問題**：
- 訓練データに過度に適合
- 汎化性能の低下

**対策**：
- 正則化の適用
- 交差検証の実施
- 特徴量選択

### 7.3 特徴量の選択

**問題**：
- 無関係な特徴量の混入
- 多重共線性

**対策**：
- 特徴量重要度の確認
- 相関分析
- 次元削減手法の適用

## 8. 評価指標

### 8.1 基本的な指標

**Accuracy（正解率）**：
```
Accuracy = (TP + TN) / (TP + TN + FP + FN)
```

**Precision（適合率）**：
```
Precision = TP / (TP + FP)
```

**Recall（再現率）**：
```
Recall = TP / (TP + FN)
```

**F1-score**：
```
F1 = 2 × (Precision × Recall) / (Precision + Recall)
```

### 8.2 混同行列

```
                予測
            0       1
実際  0    TN      FP
     1    FN      TP
```

- TP（True Positive）：正しく正例と予測
- TN（True Negative）：正しく負例と予測
- FP（False Positive）：間違って正例と予測
- FN（False Negative）：間違って負例と予測

## 9. まとめ

ロジスティック回帰は、分類問題を解決するための強力な手法です。線形回帰の概念を拡張し、確率的な出力を提供することで、実務での意思決定に役立ちます。

**主な特徴**：
- 確率的な出力
- 線形決定境界
- 解釈しやすい係数
- 計算効率が良い

**適用場面**：
- 二値分類問題
- 特徴量が数値の場合
- 解釈可能性が重要な場合

**次のステップ**：
- 多クラス分類への拡張
- 評価指標の詳細理解
- より高度な分類手法の学習

## 関連トピック

- [多クラス分類](../09_multiclass_classification/09_multiclass_classification.md)
- [分類評価指標](../10_classification_metrics/10_classification_metrics.md)
- [ROC/AUC](../11_roc_auc/11_roc_auc.md)
