{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 最急降下法の実装\n",
        "\n",
        "このノートブックでは、線形回帰における最急降下法を段階的に実装していきます。\n",
        "\n",
        "## 目次\n",
        "1. [Step 1: データの準備](#step-1-データの準備)\n",
        "2. [Step 2: 目的関数の実装](#step-2-目的関数の実装)\n",
        "3. [Step 3: 勾配の計算](#step-3-勾配の計算)\n",
        "4. [Step 4: パラメータ更新](#step-4-パラメータ更新)\n",
        "5. [Step 5: 収束判定](#step-5-収束判定)\n",
        "6. [Step 6: 完全なアルゴリズム](#step-6-完全なアルゴリズム)\n",
        "7. [実データでの検証](#実データでの検証)\n",
        "8. [演習問題](#演習問題)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 必要なライブラリのインポート\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import seaborn as sns\n",
        "\n",
        "# 日本語フォントの設定\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# 乱数の固定\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"ライブラリのインポートが完了しました。\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: データの準備\n",
        "\n",
        "まず、線形回帰のためのサンプルデータを生成します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# サンプルデータの生成\n",
        "# 住宅価格予測の例：面積から価格を予測\n",
        "np.random.seed(42)\n",
        "\n",
        "# 住宅の面積（m²）\n",
        "area = np.random.uniform(30, 120, 100)\n",
        "\n",
        "# 価格（万円）= 50万円/m² + ノイズ\n",
        "price = 50 * area + np.random.normal(0, 100, 100)\n",
        "\n",
        "# データフレームに変換\n",
        "data = pd.DataFrame({\n",
        "    'area': area,\n",
        "    'price': price\n",
        "})\n",
        "\n",
        "print(\"データの基本統計:\")\n",
        "print(data.describe())\n",
        "print(f\"\\nデータ形状: {data.shape}\")\n",
        "\n",
        "# データの可視化\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(data['area'], data['price'], alpha=0.6, color='blue')\n",
        "plt.xlabel('面積 (m²)')\n",
        "plt.ylabel('価格 (万円)')\n",
        "plt.title('住宅価格データ')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# 相関係数の確認\n",
        "correlation = data['area'].corr(data['price'])\n",
        "print(f\"\\n面積と価格の相関係数: {correlation:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# データの前処理\n",
        "# 特徴量（X）と目的変数（y）に分割\n",
        "X = data[['area']].values  # 2次元配列として保持\n",
        "y = data['price'].values\n",
        "\n",
        "# データの標準化（最急降下法では重要）\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
        "\n",
        "print(\"標準化前のデータ:\")\n",
        "print(f\"X - 平均: {X.mean():.2f}, 標準偏差: {X.std():.2f}\")\n",
        "print(f\"y - 平均: {y.mean():.2f}, 標準偏差: {y.std():.2f}\")\n",
        "\n",
        "print(\"\\n標準化後のデータ:\")\n",
        "print(f\"X_scaled - 平均: {X_scaled.mean():.2f}, 標準偏差: {X_scaled.std():.2f}\")\n",
        "print(f\"y_scaled - 平均: {y_scaled.mean():.2f}, 標準偏差: {y_scaled.std():.2f}\")\n",
        "\n",
        "# バイアス項（切片）の追加\n",
        "X_with_bias = np.c_[np.ones(X_scaled.shape[0]), X_scaled]\n",
        "print(f\"\\nバイアス項追加後の特徴量行列の形状: {X_with_bias.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: 目的関数の実装\n",
        "\n",
        "最急降下法で最小化する目的関数（平均二乗誤差）を実装します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_cost(X, y, theta):\n",
        "    \"\"\"\n",
        "    平均二乗誤差（MSE）を計算する関数\n",
        "    \n",
        "    Parameters:\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        特徴量行列\n",
        "    y : array-like, shape (n_samples,)\n",
        "        目的変数\n",
        "    theta : array-like, shape (n_features,)\n",
        "        回帰係数（パラメータ）\n",
        "    \n",
        "    Returns:\n",
        "    cost : float\n",
        "        平均二乗誤差\n",
        "    \"\"\"\n",
        "    m = len(y)  # サンプル数\n",
        "    predictions = X.dot(theta)  # 予測値の計算\n",
        "    cost = (1 / (2 * m)) * np.sum((predictions - y) ** 2)\n",
        "    return cost\n",
        "\n",
        "# 初期パラメータでの目的関数の値を確認\n",
        "initial_theta = np.zeros(X_with_bias.shape[1])  # [0, 0]で初期化\n",
        "initial_cost = compute_cost(X_with_bias, y_scaled, initial_theta)\n",
        "\n",
        "print(f\"初期パラメータ: {initial_theta}\")\n",
        "print(f\"初期コスト（目的関数の値）: {initial_cost:.4f}\")\n",
        "\n",
        "# 異なるパラメータでの目的関数の値を確認\n",
        "test_theta1 = np.array([0, 1])  # 切片=0, 傾き=1\n",
        "test_theta2 = np.array([1, 0])  # 切片=1, 傾き=0\n",
        "\n",
        "cost1 = compute_cost(X_with_bias, y_scaled, test_theta1)\n",
        "cost2 = compute_cost(X_with_bias, y_scaled, test_theta2)\n",
        "\n",
        "print(f\"\\nパラメータ [0, 1] でのコスト: {cost1:.4f}\")\n",
        "print(f\"パラメータ [1, 0] でのコスト: {cost2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: 勾配の計算\n",
        "\n",
        "目的関数の勾配（偏微分）を計算する関数を実装します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_gradient(X, y, theta):\n",
        "    \"\"\"\n",
        "    目的関数の勾配を計算する関数\n",
        "    \n",
        "    Parameters:\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        特徴量行列\n",
        "    y : array-like, shape (n_samples,)\n",
        "        目的変数\n",
        "    theta : array-like, shape (n_features,)\n",
        "        回帰係数（パラメータ）\n",
        "    \n",
        "    Returns:\n",
        "    gradient : array-like, shape (n_features,)\n",
        "        勾配ベクトル\n",
        "    \"\"\"\n",
        "    m = len(y)  # サンプル数\n",
        "    predictions = X.dot(theta)  # 予測値の計算\n",
        "    error = predictions - y  # 誤差\n",
        "    gradient = (1 / m) * X.T.dot(error)  # 勾配の計算\n",
        "    return gradient\n",
        "\n",
        "# 初期パラメータでの勾配を確認\n",
        "initial_gradient = compute_gradient(X_with_bias, y_scaled, initial_theta)\n",
        "print(f\"初期パラメータでの勾配: {initial_gradient}\")\n",
        "\n",
        "# 勾配の意味を理解するための可視化\n",
        "def plot_cost_surface():\n",
        "    \"\"\"目的関数の表面を可視化\"\"\"\n",
        "    # パラメータの範囲を設定\n",
        "    theta0_range = np.linspace(-2, 2, 50)\n",
        "    theta1_range = np.linspace(-2, 2, 50)\n",
        "    \n",
        "    # グリッドを作成\n",
        "    Theta0, Theta1 = np.meshgrid(theta0_range, theta1_range)\n",
        "    \n",
        "    # 各点でのコストを計算\n",
        "    costs = np.zeros_like(Theta0)\n",
        "    for i in range(len(theta0_range)):\n",
        "        for j in range(len(theta1_range)):\n",
        "            theta = np.array([Theta0[i, j], Theta1[i, j]])\n",
        "            costs[i, j] = compute_cost(X_with_bias, y_scaled, theta)\n",
        "    \n",
        "    # 等高線プロット\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    contour = plt.contour(Theta0, Theta1, costs, levels=20)\n",
        "    plt.clabel(contour, inline=True, fontsize=8)\n",
        "    plt.xlabel('θ₀ (切片)')\n",
        "    plt.ylabel('θ₁ (傾き)')\n",
        "    plt.title('目的関数の等高線')\n",
        "    plt.colorbar()\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.contourf(Theta0, Theta1, costs, levels=20, cmap='viridis')\n",
        "    plt.xlabel('θ₀ (切片)')\n",
        "    plt.ylabel('θ₁ (傾き)')\n",
        "    plt.title('目的関数の表面（カラーマップ）')\n",
        "    plt.colorbar()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_cost_surface()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: パラメータ更新\n",
        "\n",
        "勾配を使ってパラメータを更新する関数を実装します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_parameters(theta, gradient, learning_rate):\n",
        "    \"\"\"\n",
        "    パラメータを更新する関数\n",
        "    \n",
        "    Parameters:\n",
        "    theta : array-like, shape (n_features,)\n",
        "        現在のパラメータ\n",
        "    gradient : array-like, shape (n_features,)\n",
        "        勾配ベクトル\n",
        "    learning_rate : float\n",
        "        学習率\n",
        "    \n",
        "    Returns:\n",
        "    new_theta : array-like, shape (n_features,)\n",
        "        更新されたパラメータ\n",
        "    \"\"\"\n",
        "    new_theta = theta - learning_rate * gradient\n",
        "    return new_theta\n",
        "\n",
        "# 学習率の影響を確認\n",
        "learning_rates = [0.01, 0.1, 0.5, 1.0]\n",
        "theta_test = np.array([0.0, 0.0])\n",
        "gradient_test = np.array([0.5, -0.3])\n",
        "\n",
        "print(\"異なる学習率でのパラメータ更新:\")\n",
        "for lr in learning_rates:\n",
        "    new_theta = update_parameters(theta_test, gradient_test, lr)\n",
        "    print(f\"学習率 {lr:4.2f}: θ = [{new_theta[0]:6.3f}, {new_theta[1]:6.3f}]\")\n",
        "\n",
        "# 学習率の影響を可視化\n",
        "def visualize_learning_rate_effect():\n",
        "    \"\"\"学習率の影響を可視化\"\"\"\n",
        "    theta_start = np.array([1.5, 1.5])\n",
        "    gradient = np.array([-0.5, -0.3])\n",
        "    \n",
        "    plt.figure(figsize=(15, 3))\n",
        "    \n",
        "    for i, lr in enumerate([0.1, 0.5, 1.0, 2.0]):\n",
        "        plt.subplot(1, 4, i+1)\n",
        "        \n",
        "        # パラメータの更新軌跡\n",
        "        theta = theta_start.copy()\n",
        "        trajectory = [theta.copy()]\n",
        "        \n",
        "        for _ in range(10):\n",
        "            theta = update_parameters(theta, gradient, lr)\n",
        "            trajectory.append(theta.copy())\n",
        "        \n",
        "        trajectory = np.array(trajectory)\n",
        "        \n",
        "        plt.plot(trajectory[:, 0], trajectory[:, 1], 'bo-', markersize=4)\n",
        "        plt.plot(trajectory[0, 0], trajectory[0, 1], 'go', markersize=8, label='開始点')\n",
        "        plt.plot(trajectory[-1, 0], trajectory[-1, 1], 'ro', markersize=8, label='終了点')\n",
        "        \n",
        "        plt.xlabel('θ₀')\n",
        "        plt.ylabel('θ₁')\n",
        "        plt.title(f'学習率 = {lr}')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_learning_rate_effect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: 収束判定\n",
        "\n",
        "最急降下法の収束を判定する条件を実装します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_convergence(old_theta, new_theta, old_cost, new_cost, \n",
        "                     theta_tolerance=1e-6, cost_tolerance=1e-6):\n",
        "    \"\"\"\n",
        "    収束判定を行う関数\n",
        "    \n",
        "    Parameters:\n",
        "    old_theta : array-like\n",
        "        前回のパラメータ\n",
        "    new_theta : array-like\n",
        "        現在のパラメータ\n",
        "    old_cost : float\n",
        "        前回のコスト\n",
        "    new_cost : float\n",
        "        現在のコスト\n",
        "    theta_tolerance : float\n",
        "        パラメータ変化の許容値\n",
        "    cost_tolerance : float\n",
        "        コスト変化の許容値\n",
        "    \n",
        "    Returns:\n",
        "    converged : bool\n",
        "        収束したかどうか\n",
        "    \"\"\"\n",
        "    # パラメータの変化量\n",
        "    theta_change = np.linalg.norm(new_theta - old_theta)\n",
        "    \n",
        "    # コストの変化量\n",
        "    cost_change = abs(new_cost - old_cost)\n",
        "    \n",
        "    # 収束判定\n",
        "    converged = (theta_change < theta_tolerance) or (cost_change < cost_tolerance)\n",
        "    \n",
        "    return converged, theta_change, cost_change\n",
        "\n",
        "# 収束判定のテスト\n",
        "theta_old = np.array([1.0, 0.5])\n",
        "theta_new = np.array([1.0001, 0.5001])\n",
        "cost_old = 0.1234\n",
        "cost_new = 0.1233\n",
        "\n",
        "converged, theta_change, cost_change = check_convergence(\n",
        "    theta_old, theta_new, cost_old, cost_new\n",
        ")\n",
        "\n",
        "print(f\"パラメータ変化量: {theta_change:.6f}\")\n",
        "print(f\"コスト変化量: {cost_change:.6f}\")\n",
        "print(f\"収束判定: {converged}\")\n",
        "\n",
        "# 収束判定の可視化\n",
        "def plot_convergence_criteria():\n",
        "    \"\"\"収束判定の基準を可視化\"\"\"\n",
        "    iterations = np.arange(1, 21)\n",
        "    \n",
        "    # シミュレーション：コストが指数的に減少\n",
        "    costs = 1.0 * np.exp(-0.3 * iterations) + 0.01\n",
        "    \n",
        "    # パラメータ変化量のシミュレーション\n",
        "    theta_changes = 0.1 * np.exp(-0.2 * iterations) + 0.001\n",
        "    \n",
        "    plt.figure(figsize=(12, 4))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.semilogy(iterations, costs, 'bo-', label='コスト')\n",
        "    plt.axhline(y=1e-6, color='r', linestyle='--', label='収束閾値 (1e-6)')\n",
        "    plt.xlabel('イテレーション')\n",
        "    plt.ylabel('コスト')\n",
        "    plt.title('コストの収束')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.semilogy(iterations, theta_changes, 'go-', label='パラメータ変化量')\n",
        "    plt.axhline(y=1e-6, color='r', linestyle='--', label='収束閾値 (1e-6)')\n",
        "    plt.xlabel('イテレーション')\n",
        "    plt.ylabel('パラメータ変化量')\n",
        "    plt.title('パラメータの収束')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_convergence_criteria()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: 完全なアルゴリズム\n",
        "\n",
        "これまでに実装した関数を組み合わせて、完全な最急降下法アルゴリズムを実装します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gradient_descent(X, y, learning_rate=0.01, max_iterations=1000, \n",
        "                    theta_tolerance=1e-6, cost_tolerance=1e-6, verbose=False):\n",
        "    \"\"\"\n",
        "    最急降下法の完全な実装\n",
        "    \n",
        "    Parameters:\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        特徴量行列\n",
        "    y : array-like, shape (n_samples,)\n",
        "        目的変数\n",
        "    learning_rate : float\n",
        "        学習率\n",
        "    max_iterations : int\n",
        "        最大イテレーション数\n",
        "    theta_tolerance : float\n",
        "        パラメータ変化の許容値\n",
        "    cost_tolerance : float\n",
        "        コスト変化の許容値\n",
        "    verbose : bool\n",
        "        詳細な出力をするかどうか\n",
        "    \n",
        "    Returns:\n",
        "    theta : array-like\n",
        "        最適化されたパラメータ\n",
        "    cost_history : list\n",
        "        コストの履歴\n",
        "    theta_history : list\n",
        "        パラメータの履歴\n",
        "    \"\"\"\n",
        "    # パラメータの初期化\n",
        "    theta = np.zeros(X.shape[1])\n",
        "    \n",
        "    # 履歴の記録\n",
        "    cost_history = []\n",
        "    theta_history = [theta.copy()]\n",
        "    \n",
        "    # 初期コストの計算\n",
        "    current_cost = compute_cost(X, y, theta)\n",
        "    cost_history.append(current_cost)\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"初期パラメータ: {theta}\")\n",
        "        print(f\"初期コスト: {current_cost:.6f}\")\n",
        "        print(\"-\" * 50)\n",
        "    \n",
        "    # 最急降下法のメインループ\n",
        "    for iteration in range(max_iterations):\n",
        "        # 勾配の計算\n",
        "        gradient = compute_gradient(X, y, theta)\n",
        "        \n",
        "        # パラメータの更新\n",
        "        old_theta = theta.copy()\n",
        "        theta = update_parameters(theta, gradient, learning_rate)\n",
        "        \n",
        "        # 新しいコストの計算\n",
        "        new_cost = compute_cost(X, y, theta)\n",
        "        \n",
        "        # 履歴の記録\n",
        "        cost_history.append(new_cost)\n",
        "        theta_history.append(theta.copy())\n",
        "        \n",
        "        # 収束判定\n",
        "        converged, theta_change, cost_change = check_convergence(\n",
        "            old_theta, theta, current_cost, new_cost, \n",
        "            theta_tolerance, cost_tolerance\n",
        "        )\n",
        "        \n",
        "        if verbose and (iteration + 1) % 100 == 0:\n",
        "            print(f\"イテレーション {iteration + 1:4d}: \"\n",
        "                  f\"コスト = {new_cost:.6f}, \"\n",
        "                  f\"パラメータ変化 = {theta_change:.6f}, \"\n",
        "                  f\"コスト変化 = {cost_change:.6f}\")\n",
        "        \n",
        "        # 収束した場合\n",
        "        if converged:\n",
        "            if verbose:\n",
        "                print(f\"\\n収束しました！ (イテレーション {iteration + 1})\")\n",
        "                print(f\"最終パラメータ: {theta}\")\n",
        "                print(f\"最終コスト: {new_cost:.6f}\")\n",
        "            break\n",
        "        \n",
        "        current_cost = new_cost\n",
        "    \n",
        "    # 最大イテレーションに達した場合\n",
        "    if iteration == max_iterations - 1:\n",
        "        if verbose:\n",
        "            print(f\"\\n最大イテレーション数 ({max_iterations}) に達しました。\")\n",
        "    \n",
        "    return theta, cost_history, theta_history\n",
        "\n",
        "# 最急降下法の実行\n",
        "print(\"最急降下法の実行:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "theta_optimal, cost_history, theta_history = gradient_descent(\n",
        "    X_with_bias, y_scaled, \n",
        "    learning_rate=0.1, \n",
        "    max_iterations=1000,\n",
        "    verbose=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 結果の可視化\n",
        "def plot_gradient_descent_results():\n",
        "    \"\"\"最急降下法の結果を可視化\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # 1. コストの収束\n",
        "    axes[0, 0].plot(cost_history)\n",
        "    axes[0, 0].set_xlabel('イテレーション')\n",
        "    axes[0, 0].set_ylabel('コスト')\n",
        "    axes[0, 0].set_title('コストの収束')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. パラメータの収束\n",
        "    theta_history = np.array(theta_history)\n",
        "    axes[0, 1].plot(theta_history[:, 0], label='θ₀ (切片)')\n",
        "    axes[0, 1].plot(theta_history[:, 1], label='θ₁ (傾き)')\n",
        "    axes[0, 1].set_xlabel('イテレーション')\n",
        "    axes[0, 1].set_ylabel('パラメータ値')\n",
        "    axes[0, 1].set_title('パラメータの収束')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. パラメータ空間での軌跡\n",
        "    axes[1, 0].plot(theta_history[:, 0], theta_history[:, 1], 'bo-', markersize=4)\n",
        "    axes[1, 0].plot(theta_history[0, 0], theta_history[0, 1], 'go', markersize=8, label='開始点')\n",
        "    axes[1, 0].plot(theta_history[-1, 0], theta_history[-1, 1], 'ro', markersize=8, label='終了点')\n",
        "    axes[1, 0].set_xlabel('θ₀ (切片)')\n",
        "    axes[1, 0].set_ylabel('θ₁ (傾き)')\n",
        "    axes[1, 0].set_title('パラメータ空間での軌跡')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. 最終的な回帰直線\n",
        "    # 元のスケールに戻す\n",
        "    theta_original_scale = theta_optimal.copy()\n",
        "    theta_original_scale[0] = scaler_y.inverse_transform([[theta_optimal[0]]])[0, 0]\n",
        "    theta_original_scale[1] = theta_optimal[1] * scaler_y.scale_[0] / scaler_X.scale_[0]\n",
        "    \n",
        "    # 予測直線の描画\n",
        "    x_range = np.linspace(data['area'].min(), data['area'].max(), 100)\n",
        "    y_pred = theta_original_scale[0] + theta_original_scale[1] * x_range\n",
        "    \n",
        "    axes[1, 1].scatter(data['area'], data['price'], alpha=0.6, color='blue', label='データ')\n",
        "    axes[1, 1].plot(x_range, y_pred, 'r-', linewidth=2, label='回帰直線')\n",
        "    axes[1, 1].set_xlabel('面積 (m²)')\n",
        "    axes[1, 1].set_ylabel('価格 (万円)')\n",
        "    axes[1, 1].set_title('最終的な回帰直線')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_gradient_descent_results()\n",
        "\n",
        "# 結果の詳細表示\n",
        "print(f\"\\n最適化されたパラメータ:\")\n",
        "print(f\"切片 (β₀): {theta_optimal[0]:.6f}\")\n",
        "print(f\"傾き (β₁): {theta_optimal[1]:.6f}\")\n",
        "\n",
        "print(f\"\\n収束までのイテレーション数: {len(cost_history) - 1}\")\n",
        "print(f\"最終コスト: {cost_history[-1]:.6f}\")\n",
        "print(f\"初期コスト: {cost_history[0]:.6f}\")\n",
        "print(f\"コスト改善率: {(cost_history[0] - cost_history[-1]) / cost_history[0] * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 実データでの検証\n",
        "\n",
        "scikit-learnの線形回帰と比較して、実装の正確性を検証します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# scikit-learnとの比較\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# scikit-learnの線形回帰\n",
        "lr_sklearn = LinearRegression()\n",
        "lr_sklearn.fit(X_scaled, y_scaled)\n",
        "\n",
        "# 結果の比較\n",
        "print(\"実装の比較:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"最急降下法 - 切片: {theta_optimal[0]:.6f}, 傾き: {theta_optimal[1]:.6f}\")\n",
        "print(f\"scikit-learn - 切片: {lr_sklearn.intercept_:.6f}, 傾き: {lr_sklearn.coef_[0]:.6f}\")\n",
        "\n",
        "# 予測の比較\n",
        "y_pred_gd = X_with_bias.dot(theta_optimal)\n",
        "y_pred_sklearn = lr_sklearn.predict(X_scaled)\n",
        "\n",
        "# 予測精度の比較\n",
        "mse_gd = mean_squared_error(y_scaled, y_pred_gd)\n",
        "mse_sklearn = mean_squared_error(y_scaled, y_pred_sklearn)\n",
        "\n",
        "print(f\"\\n予測精度の比較:\")\n",
        "print(f\"最急降下法 - MSE: {mse_gd:.6f}\")\n",
        "print(f\"scikit-learn - MSE: {mse_sklearn:.6f}\")\n",
        "print(f\"差: {abs(mse_gd - mse_sklearn):.8f}\")\n",
        "\n",
        "# 学習率の影響を調査\n",
        "def compare_learning_rates():\n",
        "    \"\"\"異なる学習率での性能比較\"\"\"\n",
        "    learning_rates = [0.001, 0.01, 0.1, 0.5, 1.0]\n",
        "    \n",
        "    plt.figure(figsize=(15, 5))\n",
        "    \n",
        "    for i, lr in enumerate(learning_rates):\n",
        "        theta_lr, cost_hist, _ = gradient_descent(\n",
        "            X_with_bias, y_scaled, \n",
        "            learning_rate=lr, \n",
        "            max_iterations=1000,\n",
        "            verbose=False\n",
        "        )\n",
        "        \n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.plot(cost_hist, label=f'LR={lr}')\n",
        "        plt.xlabel('イテレーション')\n",
        "        plt.ylabel('コスト')\n",
        "        plt.title('学習率の影響')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 収束速度の比較\n",
        "    convergence_iterations = []\n",
        "    for lr in learning_rates:\n",
        "        _, cost_hist, _ = gradient_descent(\n",
        "            X_with_bias, y_scaled, \n",
        "            learning_rate=lr, \n",
        "            max_iterations=1000,\n",
        "            verbose=False\n",
        "        )\n",
        "        convergence_iterations.append(len(cost_hist) - 1)\n",
        "    \n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.bar(range(len(learning_rates)), convergence_iterations)\n",
        "    plt.xlabel('学習率のインデックス')\n",
        "    plt.ylabel('収束までのイテレーション数')\n",
        "    plt.title('収束速度の比較')\n",
        "    plt.xticks(range(len(learning_rates)), [f'{lr}' for lr in learning_rates])\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 最終コストの比較\n",
        "    final_costs = []\n",
        "    for lr in learning_rates:\n",
        "        _, cost_hist, _ = gradient_descent(\n",
        "            X_with_bias, y_scaled, \n",
        "            learning_rate=lr, \n",
        "            max_iterations=1000,\n",
        "            verbose=False\n",
        "        )\n",
        "        final_costs.append(cost_hist[-1])\n",
        "    \n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.bar(range(len(learning_rates)), final_costs)\n",
        "    plt.xlabel('学習率のインデックス')\n",
        "    plt.ylabel('最終コスト')\n",
        "    plt.title('最終コストの比較')\n",
        "    plt.xticks(range(len(learning_rates)), [f'{lr}' for lr in learning_rates])\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return learning_rates, convergence_iterations, final_costs\n",
        "\n",
        "learning_rates, iterations, costs = compare_learning_rates()\n",
        "\n",
        "# 結果のまとめ\n",
        "print(f\"\\n学習率の影響:\")\n",
        "print(\"学習率 | 収束イテレーション | 最終コスト\")\n",
        "print(\"-\" * 40)\n",
        "for lr, iter_count, final_cost in zip(learning_rates, iterations, costs):\n",
        "    print(f\"{lr:6.3f} | {iter_count:15d} | {final_cost:10.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 演習問題\n",
        "\n",
        "### 問題1: 学習率の最適化\n",
        "異なる学習率で最急降下法を実行し、最適な学習率を見つけてください。\n",
        "\n",
        "### 問題2: 収束判定の改善\n",
        "現在の収束判定を改善し、より効率的なアルゴリズムを作成してください。\n",
        "\n",
        "### 問題3: バッチサイズの影響\n",
        "ミニバッチ最急降下法を実装し、バッチサイズの影響を調査してください。\n",
        "\n",
        "### 問題4: 正則化の追加\n",
        "L1正則化（Lasso）とL2正則化（Ridge）を最急降下法に追加してください。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## まとめ\n",
        "\n",
        "このノートブックでは、最急降下法の完全な実装を行いました：\n",
        "\n",
        "### 実装した機能\n",
        "1. **目的関数**: 平均二乗誤差の計算\n",
        "2. **勾配計算**: 偏微分による勾配の導出\n",
        "3. **パラメータ更新**: 学習率を使った反復的更新\n",
        "4. **収束判定**: パラメータとコストの変化量による判定\n",
        "5. **完全なアルゴリズム**: 全ての要素を組み合わせた実装\n",
        "\n",
        "### 重要なポイント\n",
        "- **データの標準化**: 最急降下法では必須\n",
        "- **学習率の選択**: 収束速度と安定性のバランス\n",
        "- **収束判定**: 適切な閾値の設定\n",
        "- **可視化**: アルゴリズムの動作理解\n",
        "\n",
        "### 次のステップ\n",
        "- [正規方程式の実装](./normal_equation_implementation.ipynb)\n",
        "- [基本的な線形回帰](./linear_regression_basic.ipynb)\n",
        "- [特徴量スケーリング](../02_feature_scaling/02_feature_scaling.md)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
