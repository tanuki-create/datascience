{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 正規方程式の実装\n",
        "\n",
        "このノートブックでは、線形回帰における正規方程式（Normal Equation）の実装を行います。\n",
        "\n",
        "## 目次\n",
        "1. [正規方程式の理論](#正規方程式の理論)\n",
        "2. [基本的な実装](#基本的な実装)\n",
        "3. [数値的安定性の考慮](#数値的安定性の考慮)\n",
        "4. [最急降下法との比較](#最急降下法との比較)\n",
        "5. [実データでの検証](#実データでの検証)\n",
        "6. [演習問題](#演習問題)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 必要なライブラリのインポート\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import seaborn as sns\n",
        "import time\n",
        "\n",
        "# 日本語フォントの設定\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# 乱数の固定\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"ライブラリのインポートが完了しました。\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 正規方程式の理論\n",
        "\n",
        "正規方程式は、最小二乗法の解を直接的に求める手法です。\n",
        "\n",
        "### 数学的導出\n",
        "\n",
        "**目的関数**:\n",
        "```\n",
        "J(θ) = (1/2m) × ||Xθ - y||²\n",
        "```\n",
        "\n",
        "**勾配を0とおく**:\n",
        "```\n",
        "∇J(θ) = Xᵀ(Xθ - y) = 0\n",
        "```\n",
        "\n",
        "**正規方程式**:\n",
        "```\n",
        "XᵀXθ = Xᵀy\n",
        "```\n",
        "\n",
        "**解**:\n",
        "```\n",
        "θ = (XᵀX)⁻¹Xᵀy\n",
        "```\n",
        "\n",
        "### 利点と制約\n",
        "\n",
        "**利点**:\n",
        "- 解析解が得られる\n",
        "- 反復計算が不要\n",
        "- 確定的な結果\n",
        "\n",
        "**制約**:\n",
        "- 計算量がO(n³)\n",
        "- メモリ使用量が多い\n",
        "- 多重共線性で不安定\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 基本的な実装\n",
        "\n",
        "正規方程式の基本的な実装を行います。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# サンプルデータの生成\n",
        "np.random.seed(42)\n",
        "\n",
        "# 住宅価格予測の例\n",
        "area = np.random.uniform(30, 120, 100)\n",
        "price = 50 * area + np.random.normal(0, 100, 100)\n",
        "\n",
        "# データフレームに変換\n",
        "data = pd.DataFrame({\n",
        "    'area': area,\n",
        "    'price': price\n",
        "})\n",
        "\n",
        "# 特徴量と目的変数の準備\n",
        "X = data[['area']].values\n",
        "y = data['price'].values\n",
        "\n",
        "# バイアス項の追加\n",
        "X_with_bias = np.c_[np.ones(X.shape[0]), X]\n",
        "\n",
        "print(\"データの基本情報:\")\n",
        "print(f\"サンプル数: {X.shape[0]}\")\n",
        "print(f\"特徴量数: {X.shape[1]}\")\n",
        "print(f\"バイアス項追加後の形状: {X_with_bias.shape}\")\n",
        "\n",
        "# データの可視化\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(data['area'], data['price'], alpha=0.6, color='blue')\n",
        "plt.xlabel('面積 (m²)')\n",
        "plt.ylabel('価格 (万円)')\n",
        "plt.title('住宅価格データ')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normal_equation(X, y):\n",
        "    \"\"\"\n",
        "    正規方程式を解く関数\n",
        "    \n",
        "    Parameters:\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        特徴量行列（バイアス項を含む）\n",
        "    y : array-like, shape (n_samples,)\n",
        "        目的変数\n",
        "    \n",
        "    Returns:\n",
        "    theta : array-like, shape (n_features,)\n",
        "        回帰係数\n",
        "    \"\"\"\n",
        "    # 正規方程式: θ = (X^T X)^(-1) X^T y\n",
        "    XtX = X.T.dot(X)\n",
        "    Xty = X.T.dot(y)\n",
        "    \n",
        "    # 逆行列の計算\n",
        "    try:\n",
        "        theta = np.linalg.inv(XtX).dot(Xty)\n",
        "    except np.linalg.LinAlgError:\n",
        "        print(\"警告: 行列が特異です。擬似逆行列を使用します。\")\n",
        "        theta = np.linalg.pinv(XtX).dot(Xty)\n",
        "    \n",
        "    return theta\n",
        "\n",
        "# 正規方程式の実行\n",
        "print(\"正規方程式の実行:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "start_time = time.time()\n",
        "theta_ne = normal_equation(X_with_bias, y)\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"計算時間: {end_time - start_time:.6f}秒\")\n",
        "print(f\"最適化されたパラメータ:\")\n",
        "print(f\"切片 (β₀): {theta_ne[0]:.6f}\")\n",
        "print(f\"傾き (β₁): {theta_ne[1]:.6f}\")\n",
        "\n",
        "# 予測値の計算\n",
        "y_pred = X_with_bias.dot(theta_ne)\n",
        "\n",
        "# 評価指標の計算\n",
        "mse = mean_squared_error(y, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y, y_pred)\n",
        "\n",
        "print(f\"\\n評価指標:\")\n",
        "print(f\"MSE: {mse:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"R²: {r2:.4f}\")\n",
        "\n",
        "# 結果の可視化\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# 回帰直線の描画\n",
        "plt.subplot(1, 2, 1)\n",
        "x_range = np.linspace(data['area'].min(), data['area'].max(), 100)\n",
        "y_line = theta_ne[0] + theta_ne[1] * x_range\n",
        "\n",
        "plt.scatter(data['area'], data['price'], alpha=0.6, color='blue', label='データ')\n",
        "plt.plot(x_range, y_line, 'r-', linewidth=2, label='回帰直線')\n",
        "plt.xlabel('面積 (m²)')\n",
        "plt.ylabel('価格 (万円)')\n",
        "plt.title('正規方程式による回帰直線')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 残差プロット\n",
        "plt.subplot(1, 2, 2)\n",
        "residuals = y - y_pred\n",
        "plt.scatter(y_pred, residuals, alpha=0.6, color='green')\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.xlabel('予測値')\n",
        "plt.ylabel('残差')\n",
        "plt.title('残差プロット')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 数値的安定性の考慮\n",
        "\n",
        "正規方程式では、行列の条件数や多重共線性の問題を考慮する必要があります。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def robust_normal_equation(X, y, regularization=1e-6):\n",
        "    \"\"\"\n",
        "    数値的安定性を考慮した正規方程式\n",
        "    \n",
        "    Parameters:\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        特徴量行列\n",
        "    y : array-like, shape (n_samples,)\n",
        "        目的変数\n",
        "    regularization : float\n",
        "        正則化パラメータ（Ridge正則化）\n",
        "    \n",
        "    Returns:\n",
        "    theta : array-like, shape (n_features,)\n",
        "        回帰係数\n",
        "    \"\"\"\n",
        "    # 正規方程式: θ = (X^T X + λI)^(-1) X^T y\n",
        "    XtX = X.T.dot(X)\n",
        "    Xty = X.T.dot(y)\n",
        "    \n",
        "    # 条件数の確認\n",
        "    condition_number = np.linalg.cond(XtX)\n",
        "    print(f\"行列の条件数: {condition_number:.2e}\")\n",
        "    \n",
        "    if condition_number > 1e12:\n",
        "        print(\"警告: 行列の条件数が大きすぎます。正則化を適用します。\")\n",
        "        # Ridge正則化: (X^T X + λI)^(-1) X^T y\n",
        "        n_features = XtX.shape[0]\n",
        "        regularized_XtX = XtX + regularization * np.eye(n_features)\n",
        "        theta = np.linalg.solve(regularized_XtX, Xty)\n",
        "    else:\n",
        "        # 通常の逆行列計算\n",
        "        try:\n",
        "            theta = np.linalg.solve(XtX, Xty)\n",
        "        except np.linalg.LinAlgError:\n",
        "            print(\"警告: 行列が特異です。擬似逆行列を使用します。\")\n",
        "            theta = np.linalg.pinv(XtX).dot(Xty)\n",
        "    \n",
        "    return theta\n",
        "\n",
        "# 数値的安定性のテスト\n",
        "print(\"数値的安定性のテスト:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 通常のデータでの実行\n",
        "theta_robust = robust_normal_equation(X_with_bias, y)\n",
        "print(f\"通常データでの結果: {theta_robust}\")\n",
        "\n",
        "# 多重共線性のあるデータでのテスト\n",
        "print(\"\\n多重共線性のあるデータでのテスト:\")\n",
        "X_collinear = np.column_stack([\n",
        "    np.ones(100),\n",
        "    np.random.normal(0, 1, 100),\n",
        "    np.random.normal(0, 1, 100) + 0.99 * np.random.normal(0, 1, 100)  # 高い相関\n",
        "])\n",
        "\n",
        "y_collinear = 2 * X_collinear[:, 1] + 3 * X_collinear[:, 2] + np.random.normal(0, 0.1, 100)\n",
        "\n",
        "theta_collinear = robust_normal_equation(X_collinear, y_collinear)\n",
        "print(f\"多重共線性データでの結果: {theta_collinear}\")\n",
        "\n",
        "# 条件数の可視化\n",
        "def plot_condition_number():\n",
        "    \"\"\"条件数の影響を可視化\"\"\"\n",
        "    # 異なる相関を持つデータを生成\n",
        "    correlations = [0.1, 0.5, 0.8, 0.9, 0.95, 0.99]\n",
        "    condition_numbers = []\n",
        "    \n",
        "    for corr in correlations:\n",
        "        # 相関のあるデータを生成\n",
        "        x1 = np.random.normal(0, 1, 100)\n",
        "        x2 = corr * x1 + np.sqrt(1 - corr**2) * np.random.normal(0, 1, 100)\n",
        "        \n",
        "        X_test = np.column_stack([np.ones(100), x1, x2])\n",
        "        XtX = X_test.T.dot(X_test)\n",
        "        condition_numbers.append(np.linalg.cond(XtX))\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.semilogy(correlations, condition_numbers, 'bo-', markersize=8)\n",
        "    plt.axhline(y=1e12, color='r', linestyle='--', label='危険な条件数 (1e12)')\n",
        "    plt.xlabel('特徴量間の相関')\n",
        "    plt.ylabel('条件数 (対数スケール)')\n",
        "    plt.title('相関と条件数の関係')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "    \n",
        "    return correlations, condition_numbers\n",
        "\n",
        "correlations, condition_numbers = plot_condition_number()\n",
        "\n",
        "print(f\"\\n相関と条件数の関係:\")\n",
        "for corr, cond in zip(correlations, condition_numbers):\n",
        "    print(f\"相関 {corr:4.2f}: 条件数 {cond:8.2e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 最急降下法との比較\n",
        "\n",
        "正規方程式と最急降下法の性能を比較します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 最急降下法の実装（簡略版）\n",
        "def gradient_descent_simple(X, y, learning_rate=0.01, max_iterations=1000):\n",
        "    \"\"\"簡略版の最急降下法\"\"\"\n",
        "    theta = np.zeros(X.shape[1])\n",
        "    m = len(y)\n",
        "    \n",
        "    for i in range(max_iterations):\n",
        "        predictions = X.dot(theta)\n",
        "        error = predictions - y\n",
        "        gradient = (1/m) * X.T.dot(error)\n",
        "        theta = theta - learning_rate * gradient\n",
        "        \n",
        "        # 収束判定\n",
        "        if np.linalg.norm(gradient) < 1e-6:\n",
        "            break\n",
        "    \n",
        "    return theta, i + 1\n",
        "\n",
        "# 性能比較\n",
        "def compare_methods():\n",
        "    \"\"\"正規方程式と最急降下法の比較\"\"\"\n",
        "    print(\"性能比較:\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # 正規方程式\n",
        "    start_time = time.time()\n",
        "    theta_ne = normal_equation(X_with_bias, y)\n",
        "    ne_time = time.time() - start_time\n",
        "    \n",
        "    # 最急降下法\n",
        "    start_time = time.time()\n",
        "    theta_gd, iterations = gradient_descent_simple(X_with_bias, y, learning_rate=0.01)\n",
        "    gd_time = time.time() - start_time\n",
        "    \n",
        "    # 結果の比較\n",
        "    print(f\"正規方程式:\")\n",
        "    print(f\"  計算時間: {ne_time:.6f}秒\")\n",
        "    print(f\"  パラメータ: {theta_ne}\")\n",
        "    \n",
        "    print(f\"\\n最急降下法:\")\n",
        "    print(f\"  計算時間: {gd_time:.6f}秒\")\n",
        "    print(f\"  イテレーション数: {iterations}\")\n",
        "    print(f\"  パラメータ: {theta_gd}\")\n",
        "    \n",
        "    # 精度の比較\n",
        "    y_pred_ne = X_with_bias.dot(theta_ne)\n",
        "    y_pred_gd = X_with_bias.dot(theta_gd)\n",
        "    \n",
        "    mse_ne = mean_squared_error(y, y_pred_ne)\n",
        "    mse_gd = mean_squared_error(y, y_pred_gd)\n",
        "    \n",
        "    print(f\"\\n精度比較:\")\n",
        "    print(f\"正規方程式 - MSE: {mse_ne:.6f}\")\n",
        "    print(f\"最急降下法 - MSE: {mse_gd:.6f}\")\n",
        "    print(f\"差: {abs(mse_ne - mse_gd):.8f}\")\n",
        "    \n",
        "    return theta_ne, theta_gd, ne_time, gd_time\n",
        "\n",
        "theta_ne, theta_gd, ne_time, gd_time = compare_methods()\n",
        "\n",
        "# スケーラビリティのテスト\n",
        "def scalability_test():\n",
        "    \"\"\"データサイズに対する性能の変化\"\"\"\n",
        "    sample_sizes = [100, 500, 1000, 2000, 5000]\n",
        "    ne_times = []\n",
        "    gd_times = []\n",
        "    \n",
        "    for n_samples in sample_sizes:\n",
        "        # データ生成\n",
        "        X_test = np.random.normal(0, 1, (n_samples, 1))\n",
        "        y_test = 2 * X_test[:, 0] + np.random.normal(0, 0.1, n_samples)\n",
        "        X_test_bias = np.c_[np.ones(n_samples), X_test]\n",
        "        \n",
        "        # 正規方程式\n",
        "        start_time = time.time()\n",
        "        normal_equation(X_test_bias, y_test)\n",
        "        ne_times.append(time.time() - start_time)\n",
        "        \n",
        "        # 最急降下法\n",
        "        start_time = time.time()\n",
        "        gradient_descent_simple(X_test_bias, y_test, learning_rate=0.01)\n",
        "        gd_times.append(time.time() - start_time)\n",
        "    \n",
        "    # 結果の可視化\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(sample_sizes, ne_times, 'bo-', label='正規方程式')\n",
        "    plt.plot(sample_sizes, gd_times, 'ro-', label='最急降下法')\n",
        "    plt.xlabel('サンプル数')\n",
        "    plt.ylabel('計算時間 (秒)')\n",
        "    plt.title('スケーラビリティの比較')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.loglog(sample_sizes, ne_times, 'bo-', label='正規方程式')\n",
        "    plt.loglog(sample_sizes, gd_times, 'ro-', label='最急降下法')\n",
        "    plt.xlabel('サンプル数 (対数スケール)')\n",
        "    plt.ylabel('計算時間 (対数スケール)')\n",
        "    plt.title('スケーラビリティの比較 (対数スケール)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return sample_sizes, ne_times, gd_times\n",
        "\n",
        "sample_sizes, ne_times, gd_times = scalability_test()\n",
        "\n",
        "print(f\"\\nスケーラビリティの結果:\")\n",
        "print(\"サンプル数 | 正規方程式 | 最急降下法\")\n",
        "print(\"-\" * 40)\n",
        "for n, ne_time, gd_time in zip(sample_sizes, ne_times, gd_times):\n",
        "    print(f\"{n:8d} | {ne_time:10.6f} | {gd_time:10.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 実データでの検証\n",
        "\n",
        "scikit-learnとの比較を行い、実装の正確性を検証します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# scikit-learnとの比較\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# scikit-learnの線形回帰\n",
        "lr_sklearn = LinearRegression()\n",
        "lr_sklearn.fit(X, y)\n",
        "\n",
        "# 結果の比較\n",
        "print(\"scikit-learnとの比較:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"正規方程式 - 切片: {theta_ne[0]:.6f}, 傾き: {theta_ne[1]:.6f}\")\n",
        "print(f\"scikit-learn - 切片: {lr_sklearn.intercept_:.6f}, 傾き: {lr_sklearn.coef_[0]:.6f}\")\n",
        "\n",
        "# 予測の比較\n",
        "y_pred_ne = X_with_bias.dot(theta_ne)\n",
        "y_pred_sklearn = lr_sklearn.predict(X)\n",
        "\n",
        "# 予測精度の比較\n",
        "mse_ne = mean_squared_error(y, y_pred_ne)\n",
        "mse_sklearn = mean_squared_error(y, y_pred_sklearn)\n",
        "\n",
        "print(f\"\\n予測精度の比較:\")\n",
        "print(f\"正規方程式 - MSE: {mse_ne:.6f}\")\n",
        "print(f\"scikit-learn - MSE: {mse_sklearn:.6f}\")\n",
        "print(f\"差: {abs(mse_ne - mse_sklearn):.8f}\")\n",
        "\n",
        "# パラメータの差\n",
        "param_diff = np.linalg.norm(theta_ne - np.array([lr_sklearn.intercept_, lr_sklearn.coef_[0]]))\n",
        "print(f\"パラメータの差: {param_diff:.8f}\")\n",
        "\n",
        "# 可視化\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# 回帰直線の比較\n",
        "plt.subplot(1, 3, 1)\n",
        "x_range = np.linspace(data['area'].min(), data['area'].max(), 100)\n",
        "y_ne = theta_ne[0] + theta_ne[1] * x_range\n",
        "y_sklearn = lr_sklearn.intercept_ + lr_sklearn.coef_[0] * x_range\n",
        "\n",
        "plt.scatter(data['area'], data['price'], alpha=0.6, color='blue', label='データ')\n",
        "plt.plot(x_range, y_ne, 'r-', linewidth=2, label='正規方程式')\n",
        "plt.plot(x_range, y_sklearn, 'g--', linewidth=2, label='scikit-learn')\n",
        "plt.xlabel('面積 (m²)')\n",
        "plt.ylabel('価格 (万円)')\n",
        "plt.title('回帰直線の比較')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 予測値の比較\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.scatter(y_pred_ne, y_pred_sklearn, alpha=0.6, color='purple')\n",
        "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', label='完全一致')\n",
        "plt.xlabel('正規方程式の予測値')\n",
        "plt.ylabel('scikit-learnの予測値')\n",
        "plt.title('予測値の比較')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 残差の比較\n",
        "plt.subplot(1, 3, 3)\n",
        "residuals_ne = y - y_pred_ne\n",
        "residuals_sklearn = y - y_pred_sklearn\n",
        "plt.scatter(y_pred_ne, residuals_ne, alpha=0.6, color='red', label='正規方程式')\n",
        "plt.scatter(y_pred_sklearn, residuals_sklearn, alpha=0.6, color='green', label='scikit-learn')\n",
        "plt.axhline(y=0, color='black', linestyle='--')\n",
        "plt.xlabel('予測値')\n",
        "plt.ylabel('残差')\n",
        "plt.title('残差の比較')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 統計的検定\n",
        "from scipy import stats\n",
        "\n",
        "# パラメータの差の検定\n",
        "t_stat, p_value = stats.ttest_1samp(theta_ne - np.array([lr_sklearn.intercept_, lr_sklearn.coef_[0]]), 0)\n",
        "print(f\"\\n統計的検定:\")\n",
        "print(f\"t統計量: {t_stat:.6f}\")\n",
        "print(f\"p値: {p_value:.6f}\")\n",
        "\n",
        "if p_value > 0.05:\n",
        "    print(\"結論: パラメータに有意な差はありません (p > 0.05)\")\n",
        "else:\n",
        "    print(\"結論: パラメータに有意な差があります (p ≤ 0.05)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 演習問題\n",
        "\n",
        "### 問題1: 正則化の実装\n",
        "Ridge正則化を正規方程式に追加し、正則化パラメータの影響を調査してください。\n",
        "\n",
        "### 問題2: 多重共線性の対処\n",
        "多重共線性のあるデータに対して、主成分分析（PCA）を適用してから正規方程式を解いてください。\n",
        "\n",
        "### 問題3: 大規模データでの性能\n",
        "大規模データセット（10,000サンプル以上）での正規方程式と最急降下法の性能を比較してください。\n",
        "\n",
        "### 問題4: 数値的安定性の改善\n",
        "条件数が大きい場合の数値的安定性を改善する手法を実装してください。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## まとめ\n",
        "\n",
        "このノートブックでは、正規方程式の実装を行いました：\n",
        "\n",
        "### 実装した機能\n",
        "1. **基本的な正規方程式**: 解析解の直接計算\n",
        "2. **数値的安定性**: 条件数と正則化の考慮\n",
        "3. **性能比較**: 最急降下法との比較\n",
        "4. **検証**: scikit-learnとの比較\n",
        "\n",
        "### 重要なポイント\n",
        "- **計算量**: O(n³)の計算量\n",
        "- **数値的安定性**: 条件数と多重共線性の考慮\n",
        "- **正則化**: Ridge正則化による安定化\n",
        "- **スケーラビリティ**: 大規模データでの制約\n",
        "\n",
        "### 使用場面の判断\n",
        "- **正規方程式を選ぶべき場合**: 小規模データ、確実な解が必要\n",
        "- **最急降下法を選ぶべき場合**: 大規模データ、メモリ制約がある\n",
        "\n",
        "### 次のステップ\n",
        "- [基本的な線形回帰](./linear_regression_basic.ipynb)\n",
        "- [最急降下法の実装](./gradient_descent_implementation.ipynb)\n",
        "- [特徴量スケーリング](../02_feature_scaling/02_feature_scaling.md)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
