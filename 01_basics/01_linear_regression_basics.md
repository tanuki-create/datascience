# 線形回帰の基礎

## 目次
1. [線形回帰とは](#線形回帰とは)
2. [最小二乗法](#最小二乗法)
3. [最急降下法](#最急降下法)
4. [正規方程式](#正規方程式)
5. [実装の比較](#実装の比較)
6. [まとめ](#まとめ)

## 線形回帰とは

### 定義
線形回帰（Linear Regression）は、**目的変数（従属変数）と説明変数（独立変数）の間に線形関係を仮定した統計的モデル**です。最も基本的な機械学習手法の一つで、予測と説明の両方の目的で広く使用されています。

### 数学的表現

単回帰（1つの説明変数）の場合：
```
y = β₀ + β₁x + ε
```

重回帰（複数の説明変数）の場合：
```
y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ + ε
```

ここで：
- `y`: 目的変数（予測したい値）
- `x₁, x₂, ..., xₙ`: 説明変数（特徴量）
- `β₀`: 切片（y切片）
- `β₁, β₂, ..., βₙ`: 回帰係数（傾き）
- `ε`: 誤差項（ノイズ）

### 具体例：住宅価格予測

**問題設定**: 住宅の面積から価格を予測したい

**データ例**:
| 面積(m²) | 価格(万円) |
|---------|-----------|
| 50      | 3000      |
| 60      | 3500      |
| 70      | 4000      |
| 80      | 4500      |
| 90      | 5000      |

**線形回帰モデル**: `価格 = β₀ + β₁ × 面積`

### 線形回帰の目的

1. **予測**: 新しいデータに対して目的変数の値を予測
2. **説明**: 説明変数が目的変数に与える影響を理解
3. **関係性の定量化**: 変数間の関係を数値で表現

## 最小二乗法

### 基本概念

最小二乗法（Least Squares Method）は、**実際の値と予測値の差（残差）の二乗和を最小化する**ことで最適な回帰係数を求める手法です。

### 残差の定義

残差（residual）は実際の値と予測値の差です：
```
eᵢ = yᵢ - ŷᵢ = yᵢ - (β₀ + β₁xᵢ)
```

### 目的関数

最小二乗法では、残差平方和（Sum of Squared Residuals, SSR）を最小化します：

```
SSR = Σ(yᵢ - ŷᵢ)² = Σ(yᵢ - β₀ - β₁xᵢ)²
```

### 最適解の導出

#### 単回帰の場合

回帰係数を偏微分して0とおくことで最適解を求めます：

**β₁（傾き）の最適解**:
```
β₁ = Σ(xᵢ - x̄)(yᵢ - ȳ) / Σ(xᵢ - x̄)²
```

**β₀（切片）の最適解**:
```
β₀ = ȳ - β₁x̄
```

ここで：
- `x̄`: 説明変数の平均値
- `ȳ`: 目的変数の平均値

#### 重回帰の場合

行列形式で表現すると：
```
β = (XᵀX)⁻¹Xᵀy
```

ここで：
- `X`: 説明変数の行列（n × p）
- `y`: 目的変数のベクトル（n × 1）
- `β`: 回帰係数のベクトル（p × 1）

### 最小二乗法の利点

1. **数学的に厳密**: 解析的に解が求まる
2. **計算が高速**: 一度の計算で最適解が得られる
3. **統計的性質が明確**: 不偏性、効率性などの性質が保証される

### 最小二乗法の制約

1. **外れ値に敏感**: 極端な値が結果に大きく影響
2. **線形関係の仮定**: 非線形関係には適用できない
3. **多重共線性**: 説明変数間の相関が高いと不安定

## 最急降下法

### 基本概念

最急降下法（Gradient Descent）は、**目的関数の勾配（傾き）を利用して最適解を反復的に求める**最適化手法です。大規模データや複雑なモデルで特に有効です。

### アルゴリズムの流れ

1. **初期化**: 回帰係数に初期値を設定
2. **勾配計算**: 目的関数の偏微分を計算
3. **パラメータ更新**: 勾配の逆方向にパラメータを更新
4. **収束判定**: 変化量が閾値以下になるまで繰り返し

### 数学的表現

**目的関数**（平均二乗誤差）:
```
J(β) = (1/2m) × Σ(yᵢ - ŷᵢ)²
```

**勾配**:
```
∂J/∂βⱼ = (1/m) × Σ(ŷᵢ - yᵢ) × xᵢⱼ
```

**パラメータ更新**:
```
βⱼ := βⱼ - α × ∂J/∂βⱼ
```

ここで：
- `α`: 学習率（learning rate）
- `m`: サンプル数

### 学習率の重要性

**学習率が大きすぎる場合**:
- 最適解を飛び越えて発散する可能性
- 収束しない

**学習率が小さすぎる場合**:
- 収束が遅い
- 局所最適解に陥りやすい

**適切な学習率の選択**:
- 一般的に0.01〜0.1の範囲
- グリッドサーチや学習率スケジューリングで調整

### 最急降下法の種類

#### 1. バッチ最急降下法（Batch Gradient Descent）
- 全データを使用して勾配を計算
- 安定した収束
- メモリ使用量が多い

#### 2. 確率的最急降下法（Stochastic Gradient Descent, SGD）
- 1サンプルずつ勾配を計算
- 高速だが不安定
- オンライン学習に適している

#### 3. ミニバッチ最急降下法（Mini-batch Gradient Descent）
- 小さなバッチで勾配を計算
- バッチとSGDの利点を組み合わせ
- 実用的で最もよく使用される

### 最急降下法の利点

1. **大規模データに対応**: メモリ効率が良い
2. **オンライン学習**: 新しいデータを逐次学習可能
3. **非線形モデルに拡張**: ニューラルネットワークなどに応用可能

### 最急降下法の注意点

1. **局所最適解**: 大域的最適解を保証しない
2. **ハイパーパラメータ**: 学習率の調整が必要
3. **収束判定**: 適切な停止条件の設定が重要

## 正規方程式

### 基本概念

正規方程式（Normal Equation）は、**最小二乗法の解を直接的に求める**手法です。最急降下法とは異なり、反復計算が不要で一度の計算で最適解が得られます。

### 数学的導出

**目的関数**:
```
J(β) = (1/2m) × ||Xβ - y||²
```

**勾配を0とおく**:
```
∇J(β) = Xᵀ(Xβ - y) = 0
```

**正規方程式**:
```
XᵀXβ = Xᵀy
```

**解**:
```
β = (XᵀX)⁻¹Xᵀy
```

### 正規方程式の条件

正規方程式が解を持つための条件：
1. **XᵀXが正則**: 逆行列が存在する
2. **多重共線性がない**: 説明変数が線形独立
3. **サンプル数 ≥ 特徴量数**: n ≥ p

### 正規方程式の利点

1. **解析解**: 反復計算が不要
2. **確定的**: 初期値に依存しない
3. **数学的に厳密**: 最適解が保証される

### 正規方程式の制約

1. **計算量**: O(n³)の計算量（特徴量数の3乗）
2. **メモリ使用量**: 大規模データではメモリ不足
3. **数値的安定性**: 多重共線性で不安定

### 正規方程式 vs 最急降下法

| 項目 | 正規方程式 | 最急降下法 |
|------|------------|------------|
| 計算量 | O(n³) | O(kn) |
| メモリ使用量 | 多い | 少ない |
| 収束保証 | あり | なし |
| 大規模データ | 不適 | 適 |
| 実装の複雑さ | 簡単 | 中程度 |

## 実装の比較

### 使用場面の判断基準

**正規方程式を選ぶべき場合**:
- 特徴量数が少ない（< 10,000）
- メモリが十分にある
- 確実な解が欲しい

**最急降下法を選ぶべき場合**:
- 特徴量数が多い（> 10,000）
- メモリが限られている
- オンライン学習が必要

### 実装時の注意点

1. **特徴量スケーリング**: 最急降下法では必須
2. **収束判定**: 適切な閾値の設定
3. **数値的安定性**: 正規方程式では擬似逆行列の使用
4. **並列化**: 大規模データでは並列処理の検討

## まとめ

### 線形回帰の基本概念

線形回帰は機械学習の基礎となる重要な手法です。目的変数と説明変数の間の線形関係をモデル化し、予測と説明の両方の目的で使用されます。

### 最適化手法の選択

- **正規方程式**: 小規模データ、確実な解が必要な場合
- **最急降下法**: 大規模データ、メモリ制約がある場合

### 実践的なポイント

1. **データの前処理**: 欠損値処理、外れ値検出
2. **特徴量エンジニアリング**: 適切な特徴量の選択
3. **モデル評価**: 適切な評価指標の選択
4. **汎化性能**: 過学習の回避

### 次のステップ

- [特徴量スケーリング](../02_feature_scaling/02_feature_scaling.md)
- [統計的推論](../03_statistical_inference/03_statistical_inference.md)
- [モデル評価](../04_model_evaluation/04_model_evaluation.md)

---

**関連ノートブック**:
- [最急降下法の実装](./notebooks/gradient_descent_implementation.ipynb)
- [正規方程式の実装](./notebooks/normal_equation_implementation.ipynb)
- [基本的な線形回帰](./notebooks/linear_regression_basic.ipynb)
