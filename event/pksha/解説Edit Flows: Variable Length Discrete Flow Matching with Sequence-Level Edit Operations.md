OK、じゃあさっきの内容を「専門用語の注釈付き＋感覚的なイメージ」で整理し直します。

---

## 1. ざっくり世界観のおさらい（感覚ベース）

普通の言語モデル（AR: autoregressive）はこんな感じです。

> 左から1文字（1トークン）ずつ打っていくタイピング AI
> 「今まで書いた全文」を見ながら、「次の1文字」だけ決める

一方、Edit Flows はこういうイメージです。

> すでに紙に書かれている文章を、
> 「挿入」「削除」「言い換え（置換）」をペタペタ繰り返して、
> ぐちゃぐちゃの文章 → きれいな文章 に“推敲”していく AI

* 最初は「空の文章」や「適当な文字列」からスタート
* ちょっとずつ編集していって、人間が読む文にしていく
* しかも「文章のどこを編集するか」を自由に選べる
  → 左から順番にだけじゃない

この「編集を連続的に繰り返すプロセス」を、数学的には
**「連続時間マルコフ連鎖 (CTMC) 上の流れ / Flow」**として扱っている、というのが Edit Flows のコアです。

---

## 2. 専門用語の注釈（平易＋イメージ）

### 2.1 連続時間マルコフ連鎖 (CTMC)

**定義っぽく**

* 状態が「今いる場所」
* ある状態から別の状態へ、「いつ・どこに飛ぶか」を確率で決める仕組み
* 時間は 0,1,2,… のステップではなく、「連続的に」流れる

**イメージ**

* 都市がたくさんある地図を想像してください
* あなたはどこかの都市にいて、ランダムに隣の都市へ移動する
* 「どの都市へ、どれくらいの頻度で移動しやすいか」が「レート」として決まっている

Edit Flows では:

* 「都市」 = 「ある文章（トークン列）」
* 「都市間の移動」 = 「1回の編集（挿入/削除/置換）」
* 「どの編集がどれくらい起きやすいか」がレート

---

### 2.2 レート (rate)

**定義っぽく**

* 「無限に小さい時間 h で、その遷移が起きる確率 ≒ h × レート」
* 大きいほど、頻繁にその編集が起きる

**イメージ**

* 道路の「交通量」みたいなもの

  * 車がたくさん通る道 → レートが高い
  * ほとんど通らない道 → レートが低い
* Edit Flows では:

  * 「この位置 i で挿入が起きやすいか？」 → 挿入レート λ^ins
  * 「このトークンを消しちゃうか？」 → 削除レート λ^del
  * 「このトークンを別のに言い換えるか？」 → 置換レート λ^sub

---

### 2.3 編集操作 (insert / delete / substitute)

これはわかりやすいところです。

* 挿入 (insert):

  * 文字列のある位置に新しいトークンを「差し込む」
  * 例: `猫 が いる` → 「かわいい」を挿入 → `猫 が かわいい いる`

* 削除 (delete):

  * いらないトークンを消す
  * `猫 が かわいい いる` → 「いる」を削除 → `猫 が かわいい`

* 置換 (substitute):

  * トークンを別のものに置き換える
  * `猫 が かわいい` → 「かわいい」を「眠そう」に置換 → `猫 が 眠そう`

Edit Flows は、「文章生成 = この3種類の操作を、時間とともに繰り返すこと」と捉えている。

---

### 2.4 Flow Matching / Discrete Flow Matching

**Continuous Flow Matching の雰囲気**

* 「ノイズ」分布 p から「データ」分布 q まで、
  時間 t∈[0,1] の連続的な“流れ”を設計する枠組み
* 「どの方向にどれくらい“流れ”れば、最終的にデータ分布に行くか」を学習する感じ

**離散版（Discrete Flow Matching）**

* 空間が連続ベクトルじゃなく「トークン列」などの離散状態になっているもの
* 連続的な「微小変化」ではなく、「ポンッと別の状態に飛ぶ（ジャンプ）」が基本
* それでも、「時間とともに分布が p → q に変わる」ように
  遷移のレート（どのジャンプをどれくらい起こすか）を学習する

Edit Flows はこの「離散 Flow Matching」の一種で、
「ジャンプ = 1回の編集」として設計している。

---

### 2.5 カップリング (coupling π(x₀, x₁))

**定義っぽく**

* 2つの分布 p, q から

  * 「p から1個サンプルした x₀」と
  * 「q から1個サンプルした x₁」
* をペアでサンプリングする分布 π

**イメージ**

* 「ノイズ状態」と「正解の文章」をペアで持ってくる

  * 例: x₀ = めちゃくちゃな文字列
    x₁ = ちゃんとした文章

このペアに対して:

> x₀ から x₁ に向かう“編集ルート”をどう作るか？

を Flow Matching で学習していく。

---

### 2.6 アラインメント (alignment) と ε（ブランク）

**アラインメント**

* 2つの文字列の「どの文字がどれに対応しているか」を並べたもの

例: “kitten” と “smitten” の例（論文にあったやつのイメージ）

* `- kitten`
* `smitten`

このとき、先頭の “s” は「挿入」、他は「そのまま or 置換」というふうに見なせる。

Edit Flows では:

* 単純化のために「ε（ブランク）」を使って、

  * 挿入: `(ε → a)`
  * 削除: `(a → ε)`
  * 置換: `(a → b)`
    として扱う

**イメージ**

* 2行の表を作って、

  * 上：元の文
  * 下：目標の文
* 空白（ブランク）を入れて、行数を合わせる
* そのペアごとに「何の編集に相当するか」を見る

この「アラインされた世界 Z」でなら、
「1トークン単位のマスク拡散のノリ」で CTMC を作れるので、
そこから元の文章 X に戻す、という二段構造にしている。

---

### 2.7 補助プロセス / Auxiliary process Z

**なぜ必要か（直感）**

* x₀ → x₁ に行く経路（edit の順番）は無数にある
* それを全部ちゃんと数え上げて扱うのは無理
* そこで、

  * いったん「アラインされた状態列 z₀, z₁（ブランク入り）」を考え、
  * Z 空間上で「わかりやすい CTMC」を作って学習し、
  * 最後に X 空間に押し戻す

というトリックを使っている。

イメージでいうと:

> 「目に見える文章 X だけで直接“流れ”を扱うと混沌とするので、
> 一段上の世界 Z に持ち上げて、そこで整理してから戻す」

---

### 2.8 Localized Edit Flow

**問題意識**

* 「各トークンが独立に編集される」と、

  * 長い文章の場合、編集位置がバラバラになりがち
  * 文脈的につながりのない“点在する修正”ばかりになって読みづらい

**Localized の発想**

* 「一度編集が入った近くのトークンは、連鎖的に編集されやすくする」
* つまり、

  * 一箇所直したら、その周囲もまとめて直す傾向を入れる
  * コードなら「1ブロックまとめて修正」みたいな動きになる

これも、実は Z 空間に「局所的に伝播する補助 CTMC」を入れて、
Theorem 3.1 で X に戻す、という同じパターンで作られている。

イメージ的には:

> 「編集ペンを一度置いた周辺はインクが滲んで、
> 近い場所ほど“ついでに直されやすい”ようにする」

---

## 3. Edit Flows の直感モデルまとめ

かなりざっくり感覚だけで書くと、Edit Flows は:

1. 文章生成を「最初から最後まで書く」ではなく
   「推敲を段階的に続けるプロセス」として見る
2. 推敲の1歩を「挿入・削除・言い換え (置換)」に分解
3. 「どこを、どれくらいの頻度で、どう直すか」を
   連続時間の確率過程（CTMC）として設計
4. 学習の時は、

   * 「ぐちゃぐちゃ or 空の文」x₀ と「正解の文」x₁ をペアで見て
   * それらを「行を揃えた2行表（アラインメント z₀,z₁）」に持ち上げて
   * そこ上での“編集の流れ”を学習
   * 最後に「実際の編集のレート（どこに insert/delete/sub をするか）」を学ばせる
5. 推論の時は、

   * 空 or ランダム文から始めて
   * 学んだレートに従って、時間を進めながら編集を繰り返し
   * t=1 付近で止めたものを最終文章とする

というものです。

---

## 4. もし次にやるなら

* 「この専門用語、ここまで落としてほしい」と思うものがあれば、

  * 数式抜きで、図・ストーリー中心でもう一段噛み砕けます
* あるいは、

  * HumanEval みたいなコード生成タスクを例に、「実際に1本の関数がどう編集されていくのか」という“1サンプルのストーリーベース解説”もできます

どちらに振るか、指定なしなら「コード生成の1サンプルがどう変化していくか」を漫画風に解説していきます。
