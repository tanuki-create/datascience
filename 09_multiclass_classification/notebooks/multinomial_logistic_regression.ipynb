{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 多項ロジスティック回帰\n",
        "\n",
        "このノートブックでは、多項ロジスティック回帰（ソフトマックス回帰）を実装します。\n",
        "\n",
        "## 学習目標\n",
        "- ソフトマックス関数の理解と実装\n",
        "- 多項ロジスティック回帰の理論\n",
        "- 交差エントロピー損失の計算\n",
        "- 勾配降下法による最適化\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 必要なライブラリのインポート\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification, load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 日本語フォントの設定\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "sns.set_style(\"whitegrid\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. ソフトマックス関数の実装\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def softmax(z):\n",
        "    \"\"\"\n",
        "    ソフトマックス関数の実装\n",
        "    \n",
        "    Parameters:\n",
        "    z: 線形結合の結果 (n_samples, n_classes)\n",
        "    \n",
        "    Returns:\n",
        "    softmax(z): ソフトマックス関数の値\n",
        "    \"\"\"\n",
        "    # 数値的安定性のため、最大値を引く\n",
        "    z_shifted = z - np.max(z, axis=1, keepdims=True)\n",
        "    exp_z = np.exp(z_shifted)\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "# ソフトマックス関数の可視化\n",
        "z_example = np.array([[1, 2, 3], [0, 0, 0], [-1, 0, 1]])\n",
        "softmax_example = softmax(z_example)\n",
        "\n",
        "print(\"=== ソフトマックス関数の例 ===\")\n",
        "print(f\"入力 z: {z_example}\")\n",
        "print(f\"ソフトマックス出力: {softmax_example}\")\n",
        "print(f\"各行の合計: {softmax_example.sum(axis=1)}\")\n",
        "\n",
        "# ソフトマックス関数の特性を可視化\n",
        "z_range = np.linspace(-10, 10, 100)\n",
        "z_2d = np.column_stack([z_range, np.zeros_like(z_range), -z_range])\n",
        "softmax_2d = softmax(z_2d)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(z_range, softmax_2d[:, 0], 'r-', label='Class 0', linewidth=2)\n",
        "plt.plot(z_range, softmax_2d[:, 1], 'g-', label='Class 1', linewidth=2)\n",
        "plt.plot(z_range, softmax_2d[:, 2], 'b-', label='Class 2', linewidth=2)\n",
        "plt.xlabel('z[0]')\n",
        "plt.ylabel('Softmax Probability')\n",
        "plt.title('Softmax Function for 3 Classes')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(z_range, softmax_2d.sum(axis=1), 'k-', linewidth=2)\n",
        "plt.xlabel('z[0]')\n",
        "plt.ylabel('Sum of Probabilities')\n",
        "plt.title('Sum of Softmax Probabilities (should be 1)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 多項ロジスティック回帰の実装\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultinomialLogisticRegression:\n",
        "    \"\"\"\n",
        "    多項ロジスティック回帰の実装\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, learning_rate=0.01, max_iterations=1000, tolerance=1e-6):\n",
        "        \"\"\"\n",
        "        パラメータの初期化\n",
        "        \n",
        "        Parameters:\n",
        "        learning_rate: 学習率\n",
        "        max_iterations: 最大反復回数\n",
        "        tolerance: 収束判定の閾値\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.cost_history = []\n",
        "        self.classes = None\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        モデルの訓練\n",
        "        \n",
        "        Parameters:\n",
        "        X: 特徴量\n",
        "        y: ラベル\n",
        "        \"\"\"\n",
        "        # クラスの取得\n",
        "        self.classes = np.unique(y)\n",
        "        n_classes = len(self.classes)\n",
        "        n_samples, n_features = X.shape\n",
        "        \n",
        "        # パラメータの初期化\n",
        "        self.weights = np.zeros((n_classes, n_features))\n",
        "        self.bias = np.zeros(n_classes)\n",
        "        \n",
        "        # 勾配降下法\n",
        "        for i in range(self.max_iterations):\n",
        "            # 線形結合の計算\n",
        "            z = np.dot(X, self.weights.T) + self.bias\n",
        "            \n",
        "            # ソフトマックス関数の適用\n",
        "            probabilities = softmax(z)\n",
        "            \n",
        "            # 損失の計算\n",
        "            cost = self._cross_entropy_loss(y, probabilities)\n",
        "            self.cost_history.append(cost)\n",
        "            \n",
        "            # 勾配の計算\n",
        "            dw, db = self._compute_gradients(X, y, probabilities)\n",
        "            \n",
        "            # パラメータの更新\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "            \n",
        "            # 収束判定\n",
        "            if i > 0 and abs(self.cost_history[-1] - self.cost_history[-2]) < self.tolerance:\n",
        "                print(f\"収束しました。反復回数: {i+1}\")\n",
        "                break\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def _cross_entropy_loss(self, y, probabilities):\n",
        "        \"\"\"\n",
        "        交差エントロピー損失の計算\n",
        "        \n",
        "        Parameters:\n",
        "        y: 実際のラベル\n",
        "        probabilities: 予測確率\n",
        "        \n",
        "        Returns:\n",
        "        loss: 交差エントロピー損失\n",
        "        \"\"\"\n",
        "        # ワンホットエンコーディング\n",
        "        y_one_hot = np.eye(len(self.classes))[y]\n",
        "        \n",
        "        # 数値的安定性のため、確率をクリップ\n",
        "        probabilities = np.clip(probabilities, 1e-15, 1 - 1e-15)\n",
        "        \n",
        "        # 交差エントロピー損失の計算\n",
        "        loss = -np.mean(np.sum(y_one_hot * np.log(probabilities), axis=1))\n",
        "        return loss\n",
        "    \n",
        "    def _compute_gradients(self, X, y, probabilities):\n",
        "        \"\"\"\n",
        "        勾配の計算\n",
        "        \n",
        "        Parameters:\n",
        "        X: 特徴量\n",
        "        y: ラベル\n",
        "        probabilities: 予測確率\n",
        "        \n",
        "        Returns:\n",
        "        dw: 重みの勾配\n",
        "        db: バイアスの勾配\n",
        "        \"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        n_classes = len(self.classes)\n",
        "        \n",
        "        # ワンホットエンコーディング\n",
        "        y_one_hot = np.eye(n_classes)[y]\n",
        "        \n",
        "        # 勾配の計算\n",
        "        dw = np.dot((probabilities - y_one_hot).T, X) / n_samples\n",
        "        db = np.mean(probabilities - y_one_hot, axis=0)\n",
        "        \n",
        "        return dw, db\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        確率の予測\n",
        "        \n",
        "        Parameters:\n",
        "        X: 特徴量\n",
        "        \n",
        "        Returns:\n",
        "        probabilities: 予測確率\n",
        "        \"\"\"\n",
        "        z = np.dot(X, self.weights.T) + self.bias\n",
        "        return softmax(z)\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        クラスの予測\n",
        "        \n",
        "        Parameters:\n",
        "        X: 特徴量\n",
        "        \n",
        "        Returns:\n",
        "        predictions: 予測クラス\n",
        "        \"\"\"\n",
        "        probabilities = self.predict_proba(X)\n",
        "        return self.classes[np.argmax(probabilities, axis=1)]\n",
        "    \n",
        "    def score(self, X, y):\n",
        "        \"\"\"\n",
        "        精度の計算\n",
        "        \n",
        "        Parameters:\n",
        "        X: 特徴量\n",
        "        y: ラベル\n",
        "        \n",
        "        Returns:\n",
        "        accuracy: 精度\n",
        "        \"\"\"\n",
        "        predictions = self.predict(X)\n",
        "        return accuracy_score(y, predictions)\n",
        "\n",
        "# データの準備\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=2,\n",
        "    n_redundant=0,\n",
        "    n_informative=2,\n",
        "    n_clusters_per_class=1,\n",
        "    n_classes=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# データの分割と標準化\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"=== データセットの情報 ===\")\n",
        "print(f\"訓練データの形状: {X_train_scaled.shape}\")\n",
        "print(f\"テストデータの形状: {X_test_scaled.shape}\")\n",
        "print(f\"クラス数: {len(np.unique(y))}\")\n",
        "print(f\"クラスの分布: {np.bincount(y)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. モデルの訓練と評価\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# モデルの訓練\n",
        "model = MultinomialLogisticRegression(learning_rate=0.1, max_iterations=1000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 損失の履歴を可視化\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(model.cost_history)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Cost')\n",
        "plt.title('Cost History During Training')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(f\"最終的な損失: {model.cost_history[-1]:.4f}\")\n",
        "print(f\"重みの形状: {model.weights.shape}\")\n",
        "print(f\"バイアスの形状: {model.bias.shape}\")\n",
        "\n",
        "# 予測の実行\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "y_pred_proba = model.predict_proba(X_test_scaled)\n",
        "\n",
        "# 精度の計算\n",
        "accuracy = model.score(X_test_scaled, y_test)\n",
        "\n",
        "print(f\"\\n=== モデルの評価結果 ===\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# 混同行列の可視化\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Class 0', 'Class 1', 'Class 2'], \n",
        "            yticklabels=['Class 0', 'Class 1', 'Class 2'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "\n",
        "# 分類レポート\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.text(0.1, 0.9, 'Classification Report:', fontsize=14, fontweight='bold', transform=plt.gca().transAxes)\n",
        "plt.text(0.1, 0.8, classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1', 'Class 2']), \n",
        "         fontsize=10, transform=plt.gca().transAxes, verticalalignment='top')\n",
        "plt.axis('off')\n",
        "plt.title('Classification Report')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. scikit-learnとの比較\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# scikit-learnの多項ロジスティック回帰\n",
        "sklearn_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=42, max_iter=1000)\n",
        "sklearn_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 予測\n",
        "sklearn_pred = sklearn_model.predict(X_test_scaled)\n",
        "sklearn_proba = sklearn_model.predict_proba(X_test_scaled)\n",
        "\n",
        "# 精度の比較\n",
        "sklearn_accuracy = sklearn_model.score(X_test_scaled, y_test)\n",
        "\n",
        "print(\"=== scikit-learnとの比較 ===\")\n",
        "print(f\"自作モデルの精度: {accuracy:.4f}\")\n",
        "print(f\"scikit-learnの精度: {sklearn_accuracy:.4f}\")\n",
        "print(f\"精度の差: {abs(accuracy - sklearn_accuracy):.4f}\")\n",
        "\n",
        "# 係数の比較\n",
        "print(f\"\\n=== 係数の比較 ===\")\n",
        "print(f\"自作モデルの重み: {model.weights}\")\n",
        "print(f\"scikit-learnの重み: {sklearn_model.coef_}\")\n",
        "print(f\"自作モデルのバイアス: {model.bias}\")\n",
        "print(f\"scikit-learnのバイアス: {sklearn_model.intercept_}\")\n",
        "\n",
        "# 予測確率の比較\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "for i in range(3):\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    plt.scatter(y_pred_proba[:, i], sklearn_proba[:, i], alpha=0.6)\n",
        "    plt.plot([0, 1], [0, 1], 'r--', alpha=0.8)\n",
        "    plt.xlabel('自作モデルの予測確率')\n",
        "    plt.ylabel('scikit-learnの予測確率')\n",
        "    plt.title(f'Class {i} Probability Comparison')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 演習問題\n",
        "\n",
        "### 演習1: 学習率の影響\n",
        "異なる学習率でモデルを訓練し、収束の様子を比較してみましょう。\n",
        "\n",
        "### 演習2: 正則化の実装\n",
        "L2正則化を追加して、過学習を防ぐ実装を作成してみましょう。\n",
        "\n",
        "### 演習3: クラス数の影響\n",
        "異なるクラス数でデータセットを生成し、多項ロジスティック回帰の性能を確認してみましょう。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## まとめ\n",
        "\n",
        "このノートブックでは、多項ロジスティック回帰（ソフトマックス回帰）を実装しました。\n",
        "\n",
        "**学習した内容**：\n",
        "- ソフトマックス関数の実装と特性\n",
        "- 多項ロジスティック回帰の理論\n",
        "- 交差エントロピー損失の計算\n",
        "- 勾配降下法による最適化\n",
        "- scikit-learnとの比較\n",
        "\n",
        "**重要なポイント**：\n",
        "- ソフトマックス関数は確率の正規化を自動的に行う\n",
        "- 数値的安定性に注意が必要\n",
        "- 多項ロジスティック回帰は1つのモデルで全クラスを同時に扱う\n",
        "- 勾配の計算が複雑になる\n",
        "\n",
        "**次のステップ**：\n",
        "- 正則化の実装\n",
        "- より高度な最適化手法の学習\n",
        "- 深層学習への応用\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
