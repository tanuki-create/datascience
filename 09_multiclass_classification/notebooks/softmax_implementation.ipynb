{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ソフトマックス関数と損失関数\n",
        "\n",
        "このノートブックでは、ソフトマックス関数と多クラス分類の損失関数について詳しく学習します。\n",
        "\n",
        "## 学習目標\n",
        "- ソフトマックス関数の数学的性質\n",
        "- 交差エントロピー損失の導出\n",
        "- 勾配の計算\n",
        "- 数値的安定性の考慮\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 必要なライブラリのインポート\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.special import softmax as scipy_softmax\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 日本語フォントの設定\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "sns.set_style(\"whitegrid\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. ソフトマックス関数の実装と比較\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def softmax_naive(z):\n",
        "    \"\"\"\n",
        "    ナイーブなソフトマックス関数の実装（数値的不安定）\n",
        "    \n",
        "    Parameters:\n",
        "    z: 線形結合の結果\n",
        "    \n",
        "    Returns:\n",
        "    softmax(z): ソフトマックス関数の値\n",
        "    \"\"\"\n",
        "    exp_z = np.exp(z)\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "def softmax_stable(z):\n",
        "    \"\"\"\n",
        "    数値的に安定なソフトマックス関数の実装\n",
        "    \n",
        "    Parameters:\n",
        "    z: 線形結合の結果\n",
        "    \n",
        "    Returns:\n",
        "    softmax(z): ソフトマックス関数の値\n",
        "    \"\"\"\n",
        "    # 数値的安定性のため、最大値を引く\n",
        "    z_shifted = z - np.max(z, axis=1, keepdims=True)\n",
        "    exp_z = np.exp(z_shifted)\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "# テストケース\n",
        "z_test = np.array([[1, 2, 3], [0, 0, 0], [-1, 0, 1], [10, 20, 30]])\n",
        "\n",
        "print(\"=== ソフトマックス関数の比較 ===\")\n",
        "print(f\"入力 z: {z_test}\")\n",
        "\n",
        "# 各実装の結果\n",
        "naive_result = softmax_naive(z_test)\n",
        "stable_result = softmax_stable(z_test)\n",
        "scipy_result = scipy_softmax(z_test, axis=1)\n",
        "\n",
        "print(f\"\\nナイーブ実装: {naive_result}\")\n",
        "print(f\"安定実装: {stable_result}\")\n",
        "print(f\"SciPy実装: {scipy_result}\")\n",
        "\n",
        "print(f\"\\n各行の合計:\")\n",
        "print(f\"ナイーブ: {naive_result.sum(axis=1)}\")\n",
        "print(f\"安定: {stable_result.sum(axis=1)}\")\n",
        "print(f\"SciPy: {scipy_result.sum(axis=1)}\")\n",
        "\n",
        "# 数値的不安定性の例\n",
        "z_large = np.array([[100, 101, 102]])\n",
        "print(f\"\\n=== 数値的不安定性の例 ===\")\n",
        "print(f\"大きな値の入力: {z_large}\")\n",
        "\n",
        "try:\n",
        "    naive_large = softmax_naive(z_large)\n",
        "    print(f\"ナイーブ実装: {naive_large}\")\n",
        "except:\n",
        "    print(\"ナイーブ実装: オーバーフローエラー\")\n",
        "\n",
        "stable_large = softmax_stable(z_large)\n",
        "scipy_large = scipy_softmax(z_large, axis=1)\n",
        "print(f\"安定実装: {stable_large}\")\n",
        "print(f\"SciPy実装: {scipy_large}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. ソフトマックス関数の可視化\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ソフトマックス関数の可視化\n",
        "z_range = np.linspace(-10, 10, 100)\n",
        "z_2d = np.column_stack([z_range, np.zeros_like(z_range), -z_range])\n",
        "softmax_2d = softmax_stable(z_2d)\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(z_range, softmax_2d[:, 0], 'r-', label='Class 0', linewidth=2)\n",
        "plt.plot(z_range, softmax_2d[:, 1], 'g-', label='Class 1', linewidth=2)\n",
        "plt.plot(z_range, softmax_2d[:, 2], 'b-', label='Class 2', linewidth=2)\n",
        "plt.xlabel('z[0]')\n",
        "plt.ylabel('Softmax Probability')\n",
        "plt.title('Softmax Function for 3 Classes')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(z_range, softmax_2d.sum(axis=1), 'k-', linewidth=2)\n",
        "plt.xlabel('z[0]')\n",
        "plt.ylabel('Sum of Probabilities')\n",
        "plt.title('Sum of Softmax Probabilities (should be 1)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "# 3D可視化の準備\n",
        "z1_range = np.linspace(-5, 5, 50)\n",
        "z2_range = np.linspace(-5, 5, 50)\n",
        "Z1, Z2 = np.meshgrid(z1_range, z2_range)\n",
        "Z3 = np.zeros_like(Z1)\n",
        "\n",
        "z_3d = np.stack([Z1.ravel(), Z2.ravel(), Z3.ravel()], axis=1)\n",
        "softmax_3d = softmax_stable(z_3d)\n",
        "softmax_3d = softmax_3d.reshape(Z1.shape + (3,))\n",
        "\n",
        "plt.contourf(Z1, Z2, softmax_3d[:, :, 0], levels=20, alpha=0.8, cmap='Reds')\n",
        "plt.colorbar(label='Class 0 Probability')\n",
        "plt.xlabel('z[0]')\n",
        "plt.ylabel('z[1]')\n",
        "plt.title('Class 0 Probability (z[2] = 0)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 交差エントロピー損失の実装\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    交差エントロピー損失の計算\n",
        "    \n",
        "    Parameters:\n",
        "    y_true: 実際のラベル (n_samples,)\n",
        "    y_pred: 予測確率 (n_samples, n_classes)\n",
        "    \n",
        "    Returns:\n",
        "    loss: 交差エントロピー損失\n",
        "    \"\"\"\n",
        "    n_samples = y_true.shape[0]\n",
        "    n_classes = y_pred.shape[1]\n",
        "    \n",
        "    # ワンホットエンコーディング\n",
        "    y_one_hot = np.eye(n_classes)[y_true]\n",
        "    \n",
        "    # 数値的安定性のため、確率をクリップ\n",
        "    y_pred_clipped = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "    \n",
        "    # 交差エントロピー損失の計算\n",
        "    loss = -np.mean(np.sum(y_one_hot * np.log(y_pred_clipped), axis=1))\n",
        "    return loss\n",
        "\n",
        "def cross_entropy_loss_naive(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    ナイーブな交差エントロピー損失の計算（数値的不安定）\n",
        "    \n",
        "    Parameters:\n",
        "    y_true: 実際のラベル (n_samples,)\n",
        "    y_pred: 予測確率 (n_samples, n_classes)\n",
        "    \n",
        "    Returns:\n",
        "    loss: 交差エントロピー損失\n",
        "    \"\"\"\n",
        "    n_samples = y_true.shape[0]\n",
        "    n_classes = y_pred.shape[1]\n",
        "    \n",
        "    # ワンホットエンコーディング\n",
        "    y_one_hot = np.eye(n_classes)[y_true]\n",
        "    \n",
        "    # 交差エントロピー損失の計算（クリップなし）\n",
        "    loss = -np.mean(np.sum(y_one_hot * np.log(y_pred), axis=1))\n",
        "    return loss\n",
        "\n",
        "# テストケース\n",
        "y_true_test = np.array([0, 1, 2, 0])\n",
        "y_pred_test = np.array([\n",
        "    [0.7, 0.2, 0.1],\n",
        "    [0.1, 0.8, 0.1],\n",
        "    [0.2, 0.3, 0.5],\n",
        "    [0.6, 0.3, 0.1]\n",
        "])\n",
        "\n",
        "print(\"=== 交差エントロピー損失の計算 ===\")\n",
        "print(f\"実際のラベル: {y_true_test}\")\n",
        "print(f\"予測確率: {y_pred_test}\")\n",
        "\n",
        "loss_stable = cross_entropy_loss(y_true_test, y_pred_test)\n",
        "loss_naive = cross_entropy_loss_naive(y_true_test, y_pred_test)\n",
        "\n",
        "print(f\"\\n安定実装の損失: {loss_stable:.4f}\")\n",
        "print(f\"ナイーブ実装の損失: {loss_naive:.4f}\")\n",
        "\n",
        "# 極端なケースでの比較\n",
        "y_pred_extreme = np.array([\n",
        "    [0.99, 0.005, 0.005],\n",
        "    [0.005, 0.99, 0.005],\n",
        "    [0.005, 0.005, 0.99],\n",
        "    [0.99, 0.005, 0.005]\n",
        "])\n",
        "\n",
        "print(f\"\\n=== 極端なケースでの比較 ===\")\n",
        "print(f\"極端な予測確率: {y_pred_extreme}\")\n",
        "\n",
        "loss_stable_extreme = cross_entropy_loss(y_true_test, y_pred_extreme)\n",
        "loss_naive_extreme = cross_entropy_loss_naive(y_true_test, y_pred_extreme)\n",
        "\n",
        "print(f\"安定実装の損失: {loss_stable_extreme:.4f}\")\n",
        "print(f\"ナイーブ実装の損失: {loss_naive_extreme:.4f}\")\n",
        "\n",
        "# 損失関数の可視化\n",
        "y_true_viz = np.array([0, 1, 2, 0])\n",
        "y_pred_viz = np.linspace(0.01, 0.99, 100)\n",
        "\n",
        "losses = []\n",
        "for pred in y_pred_viz:\n",
        "    y_pred_test_viz = np.array([\n",
        "        [pred, (1-pred)/2, (1-pred)/2],\n",
        "        [(1-pred)/2, pred, (1-pred)/2],\n",
        "        [(1-pred)/2, (1-pred)/2, pred],\n",
        "        [pred, (1-pred)/2, (1-pred)/2]\n",
        "    ])\n",
        "    loss = cross_entropy_loss(y_true_viz, y_pred_test_viz)\n",
        "    losses.append(loss)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(y_pred_viz, losses, 'b-', linewidth=2)\n",
        "plt.xlabel('Predicted Probability')\n",
        "plt.ylabel('Cross-Entropy Loss')\n",
        "plt.title('Cross-Entropy Loss vs Predicted Probability')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 勾配の計算\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_gradients(X, y_true, y_pred):\n",
        "    \"\"\"\n",
        "    勾配の計算\n",
        "    \n",
        "    Parameters:\n",
        "    X: 特徴量 (n_samples, n_features)\n",
        "    y_true: 実際のラベル (n_samples,)\n",
        "    y_pred: 予測確率 (n_samples, n_classes)\n",
        "    \n",
        "    Returns:\n",
        "    dw: 重みの勾配 (n_classes, n_features)\n",
        "    db: バイアスの勾配 (n_classes,)\n",
        "    \"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    n_classes = y_pred.shape[1]\n",
        "    \n",
        "    # ワンホットエンコーディング\n",
        "    y_one_hot = np.eye(n_classes)[y_true]\n",
        "    \n",
        "    # 勾配の計算\n",
        "    dw = np.dot((y_pred - y_one_hot).T, X) / n_samples\n",
        "    db = np.mean(y_pred - y_one_hot, axis=0)\n",
        "    \n",
        "    return dw, db\n",
        "\n",
        "# 勾配のテスト\n",
        "X_test = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "y_true_test = np.array([0, 1, 2])\n",
        "y_pred_test = np.array([\n",
        "    [0.7, 0.2, 0.1],\n",
        "    [0.1, 0.8, 0.1],\n",
        "    [0.2, 0.3, 0.5]\n",
        "])\n",
        "\n",
        "dw, db = compute_gradients(X_test, y_true_test, y_pred_test)\n",
        "\n",
        "print(\"=== 勾配の計算 ===\")\n",
        "print(f\"特徴量 X: {X_test}\")\n",
        "print(f\"実際のラベル: {y_true_test}\")\n",
        "print(f\"予測確率: {y_pred_test}\")\n",
        "print(f\"\\n重みの勾配 dw: {dw}\")\n",
        "print(f\"バイアスの勾配 db: {db}\")\n",
        "\n",
        "# 勾配の可視化\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(dw, cmap='RdBu', aspect='auto')\n",
        "plt.colorbar(label='Gradient Value')\n",
        "plt.title('Weight Gradients (dw)')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Classes')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(range(len(db)), db, color='skyblue', alpha=0.7)\n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Bias Gradient')\n",
        "plt.title('Bias Gradients (db)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 演習問題\n",
        "\n",
        "### 演習1: 数値的安定性の確認\n",
        "異なる実装方法でソフトマックス関数を比較し、数値的安定性の違いを確認してみましょう。\n",
        "\n",
        "### 演習2: 勾配の検証\n",
        "数値微分を使用して、計算した勾配が正しいことを確認してみましょう。\n",
        "\n",
        "### 演習3: 損失関数の最適化\n",
        "勾配降下法を使用して、損失関数を最小化してみましょう。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## まとめ\n",
        "\n",
        "このノートブックでは、ソフトマックス関数と多クラス分類の損失関数について詳しく学習しました。\n",
        "\n",
        "**学習した内容**：\n",
        "- ソフトマックス関数の実装と数値的安定性\n",
        "- 交差エントロピー損失の計算\n",
        "- 勾配の計算方法\n",
        "- 数値的不安定性への対処\n",
        "\n",
        "**重要なポイント**：\n",
        "- ソフトマックス関数は確率の正規化を自動的に行う\n",
        "- 数値的安定性のため、最大値を引くことが重要\n",
        "- 交差エントロピー損失は確率のクリップが必要\n",
        "- 勾配の計算は行列演算で効率的に実行可能\n",
        "\n",
        "**次のステップ**：\n",
        "- より高度な最適化手法の学習\n",
        "- 正則化の実装\n",
        "- 深層学習への応用\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
